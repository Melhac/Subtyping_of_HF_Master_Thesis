{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Embeddings\n",
    "This Notebook is training word2Vec embeddings (cbow and Skipgram) and evaluates them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "#import spacy\n",
    "import multiprocessing\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_data = pq.read_table('Cohort/Time_Series/all_time_series_medium_timeseries_data_per_patient.parquet').to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeseries_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert numerical list into srings\n",
    "timeseries_data_string = timeseries_data.unique_concept.apply(lambda s: list(map(str, s) ))\n",
    "timeseries_data_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in timeseries_data_string.head(): \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"Cohort/Time_Series/Medium/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params for word embedding models\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "emb_dimension = 20\n",
    "min_word_count = 0\n",
    "num_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "sentences = timeseries_data_string\n",
    "sentences\n",
    "model = Word2Vec(sentences=sentences, size=emb_dimension, window=num_window, min_count=min_word_count, workers=num_cores, sg=0)\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time in sec: ', end - start)\n",
    "\n",
    "model.save(model_dir + 'cbow_dim{}_win{}_mc{}.bin'.format(emb_dimension, num_window, min_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time_Series/Medium\n",
    "cbow_model0 = Word2Vec.load(model_dir + \"cbow_dim20_win5_mc0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary tall_time_series_woProcedures_dictionary.parquet\n",
    "dic=pq.read_table('Cohort/Time_Series/all_time_series_medium_dictionary.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the neighbours of a specific concept and merge term id with term \n",
    "near=pd.DataFrame(cbow_model0.wv.most_similar('231'), columns=[\"term_id\", \"similarity\"])\n",
    "near['term_id'] = near['term_id'].apply(pd.to_numeric)\n",
    "dic['term_id'] = dic['term_id'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display neighbours\n",
    "df_merge_col = pd.merge(near, dic, on='term_id')\n",
    "df_merge_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the embedding\n",
    "X = cbow_model0[cbow_model0.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(40,20))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "#words = list(cbow_model0.wv.vocab)\n",
    "terms=dic['Term']\n",
    "for i, word in enumerate(terms):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = Word2Vec(sentences=sentences, size=emb_dimension, window=num_window, min_count=min_word_count, workers=num_cores, sg=1)\n",
    "\n",
    "end = time.time()\n",
    "print('Processing time in sec: ', end - start)\n",
    "\n",
    "model.save(model_dir + 'skipgram_dim{}_win{}_mc{}.bin'.format(emb_dimension, num_window, min_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model0 = Word2Vec.load(model_dir + \"skipgram_dim50_win5_mc0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dictionary \n",
    "dic=pq.read_table('Cohort/Time_Series/all_time_series_woProcedures_dictionary.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the neighbours of a specific concept and merge term id with term \n",
    "near=pd.DataFrame(skipgram_model0.wv.most_similar('108'), columns=[\"term_id\", \"similarity\"])\n",
    "near['term_id'] = near['term_id'].apply(pd.to_numeric)\n",
    "dic['term_id'] = dic['term_id'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display neighbours\n",
    "df_merge_col = pd.merge(near, dic, on='term_id')\n",
    "df_merge_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the embedding\n",
    "X = skipgram_model0[skipgram_model0.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "#words = list(cbow_model0.wv.vocab)\n",
    "terms=dic['Term']\n",
    "for i, word in enumerate(terms):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# 51,000,000 rows need around 27.5k seconds\n",
    "model = FastText(sentences=sentences, size=emb_dimension, window=num_window, min_count=min_word_count, workers=num_cores)\n",
    "end = time.time()\n",
    "print('Processing time in sec: ', end - start)\n",
    "\n",
    "model.save(model_dir + 'fastText_dim{}_win{}_mc{}.bin'.format(emb_dimension, num_window, min_word_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
