{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is providing a Cluster pipeline for aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### REQUIRES THE DATAFRAME FOLDER TO BE NAMED 'Cohorts', WHICH INCLUDES ALL PRECOMPUTED DATAFRAMES #####\n",
    "import fiber\n",
    "from fiber.cohort import Cohort\n",
    "from fiber.condition import Patient, MRNs\n",
    "from fiber.condition import Diagnosis\n",
    "from fiber.condition import Measurement, Encounter, Drug, TobaccoUse,LabValue\n",
    "from fiber.storage import yaml as fiberyaml\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from functools import reduce\n",
    "from ppca import PPCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "import pickle\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import scipy.stats as stats\n",
    "import researchpy as rp\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from keras.layers import Input, Dense \n",
    "from keras.models import Model, Sequential \n",
    "from keras import regularizers \n",
    "import umap\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columnTransformer out of the scikit-learn libraby\n",
    "#applying StandardScaler on numeric values and OneHotEncouder on categorical\n",
    "def apply_columnTransformer(df,df_name,num_scaler_name,cat_scaler_name,experiment_name):\n",
    "    #load transformed df and column transformer if already available\n",
    "    name_transformed_df=df_name+'_'+num_scaler_name+'_'+cat_scaler_name\n",
    "    try:\n",
    "         \n",
    "        with open('Cohort/Models/ColumnTransformer/'+experiment_name+'.pkl', 'rb') as f:\n",
    "            ctransformer = pickle.load(f)\n",
    "        print('ctransformer loaded')\n",
    "        load = np.load('Cohort/Models/ColumnTransformer/'+experiment_name+'.npz')\n",
    "        transformed_df=load['a']\n",
    "        print('transformed_df loaded')\n",
    "            \n",
    "            \n",
    "        \n",
    "        #print(transformed_df)\n",
    "        return transformed_df, ctransformer\n",
    "    #if a new df was introduced, apply the column transformation\n",
    "    except:\n",
    "        #identify categorical and numerical Columns of the dataframe\n",
    "        categorical_cols = [c for c in df.columns if df[c].dtype in [np.object,np.str] ]\n",
    "        numerical_cols = [c for c in df.columns if df[c].dtype in [np.float, np.int] ]\n",
    "        #select the scaler that should be applied\n",
    "        if num_scaler_name=='StandardScaler':\n",
    "            num_scaler=StandardScaler()\n",
    "        if num_scaler_name=='MinMaxScaler':\n",
    "            num_scaler=preprocessing.MinMaxScaler()    \n",
    "        if cat_scaler_name=='OneHotEncoder':\n",
    "            cat_scaler=ce.OneHotEncoder()\n",
    "        if cat_scaler_name=='BinaryEncoder':\n",
    "            cat_scaler=ce.BinaryEncoder(drop_invariant = True, handle_missing = 'return_nan')   \n",
    "        #apply the Transformer\n",
    "        ctransformer = ColumnTransformer([\n",
    "        ('num', num_scaler, numerical_cols),\n",
    "        ('cat', cat_scaler, categorical_cols)])\n",
    "        #save the ctransformer and the transformed df       \n",
    "        dump(ctransformer, open('Cohort/Models/ColumnTransformer/'+experiment_name+'.pkl', 'wb')) \n",
    "        transformed_df=ctransformer.fit_transform(df)\n",
    "        print(transformed_df)\n",
    "        np.savez_compressed('Cohort/Models/ColumnTransformer/'+experiment_name+'.npz',a=transformed_df)\n",
    "        \n",
    "            \n",
    "        return transformed_df, ctransformer\n",
    "  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ppca(df,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name): \n",
    "    ppca = PPCA()\n",
    "    ppca.fit(data=transformed_df, d=dimension, verbose=True)\n",
    "    transformed_train = ppca.transform()\n",
    "    print(transformed_train)\n",
    "    dump(ppca, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb')) \n",
    "    return transformed_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_TSNE(df,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name):\n",
    "    time_start = time.time()\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=160, n_iter=1000)\n",
    "    tsne_results = tsne.fit_transform(transformed_df)\n",
    "    #tsne_results = tsne.fit_transform(df)\n",
    "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "    plt.scatter(tsne_results[:,0],tsne_results[:,1])\n",
    "    dump(tsne_results, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb'))\n",
    "    return tsne_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST ICA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ICA(df,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name):\n",
    "    transformer = FastICA(n_components=dimension,random_state=0)\n",
    "    X_transformed = transformer.fit_transform(transformed_df)\n",
    "    print(X_transformed.shape)\n",
    "    dump(transformer, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb'))\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/singular-value-decomposition-for-dimensionality-reduction-in-python/\n",
    "#good if data is sparse\n",
    "#n_iter=5 = Default\n",
    "def apply_svd(df,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name):\n",
    "    svd = TruncatedSVD(n_components=dimension, n_iter=5, random_state=42)\n",
    "    X_transformed=svd.fit_transform(transformed_df)\n",
    "    #print(X_transformed.shape)\n",
    "    dump(svd, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb'))\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(df,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name):\n",
    "    pca = PCA(n_components=dimension)\n",
    "    X_transformed=pca.fit_transform(transformed_df)\n",
    "\n",
    "    print()\n",
    "    print(X_transformed)\n",
    "    dump(pca, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb'))\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ipca(df,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name):\n",
    "    ipca = IncrementalPCA(n_components=dimension, batch_size=30)\n",
    "    X_transformed=ipca.fit_transform(transformed_df)\n",
    "    print(X_transformed)\n",
    "    dump(ipca, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb'))\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kpca(df,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name):\n",
    "    kpca = KernelPCA(n_components=dimension, kernel='linear')\n",
    "    X_transformed=kpca.fit_transform(transformed_df)\n",
    "    print(X_transformed)\n",
    "    dump(kpca, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb'))\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lda(df,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name):\n",
    "    lda = LatentDirichletAllocation(n_components=dimension,random_state=0)\n",
    "    transformed_df=abs(transformed_df)\n",
    "    X_transformed=lda.fit_transform(transformed_df)\n",
    "    print(X_transformed)\n",
    "    dump(lda, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb'))\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap(df,transformed_df,dimension,umap_distance,umap_neighbors,df_name,num_scaler_name,cat_scaler_name,experiment_name,save):\n",
    "    clusterable_embedding = umap.UMAP(\n",
    "        n_neighbors=umap_neighbors,\n",
    "        min_dist=umap_distance,\n",
    "        n_components=dimension,\n",
    "        random_state=42,\n",
    "    )\n",
    "    X_transformed=clusterable_embedding.fit_transform(transformed_df)\n",
    "    if save==True:\n",
    "        dump(clusterable_embedding, open('Cohort/Models/DimReduction/'+experiment_name+'.pkl', 'wb'))\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_AE(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,a_f_decoder,a_f_encoder,n_layer,batchsize,epochs,optimizer,loss_function):\n",
    "    X=transformed_df\n",
    "    #Building the Auto-encoder neural network\n",
    "    # Building the Input Layer \n",
    "    input_layer = Input(shape =(X.shape[1], )) \n",
    "\n",
    "    # Building the Encoder network \n",
    "    encoded = Dense(50, activation =a_f_encoder)(input_layer) \n",
    "    encoded = Dense(dimension, activation =a_f_encoder)(encoded) \n",
    "    decoded = Dense(50, activation =a_f_decoder)(encoded) \n",
    "    # Building the Output Layer \n",
    "    output_layer = Dense(X.shape[1], activation =a_f_decoder)(decoded) \n",
    "    \n",
    "    #Train AE\n",
    "    # Defining the parameters of the Auto-encoder network \n",
    "    autoencoder = Model(input_layer, output_layer) \n",
    "    autoencoder.compile(optimizer=optimizer, loss=loss_function) \n",
    "\n",
    "    # Training the Auto-encoder network \n",
    "    history = autoencoder.fit(X, X,  \n",
    "                    batch_size = batchsize, epochs = epochs,  \n",
    "                    shuffle = True, validation_split = 0.20)\n",
    "    #loss for result excel\n",
    "    print(history.history['val_loss'][(epochs-1)])\n",
    "    loss=history.history['val_loss'][(epochs-1)]\n",
    "    #Retaining the encoder part of the Auto-encoder to encode data\n",
    "\n",
    "    hidden_representation = Sequential() \n",
    "    hidden_representation.add(autoencoder.layers[0]) \n",
    "    hidden_representation.add(autoencoder.layers[1])  \n",
    "    hidden_representation.add(autoencoder.layers[2])\n",
    "    normal_hidden_rep = hidden_representation.predict(X)\n",
    "   #dump(history, open('Cohort/Models/DimReduction/'+df_name+'_'+num_scaler_name+'_'+cat_scaler_name+'_AE_'+str(dimension)+'_'+a_f_encoder+'_'+a_f_decoder+'_'+str(n_layer)+'_'+str(batchsize)+'_'+str(epochs)+'_'+optimizer+'_'+loss_function+'.pkl', 'wb'))\n",
    "    return normal_hidden_rep, loss\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeans(transformed_sample,ellbow_method,cluster,df_name,num_scaler_name,cat_scaler_name,dim_red_method,experiment_name):\n",
    "    if ellbow_method==True:\n",
    "        elbow_method(transformed_sample)\n",
    "    #scatter_plot(transformed_sample,None) \n",
    "    #plt.scatter(transformed_sample[:,0],transformed_sample[:,1])\n",
    "    kmeans = KMeans(n_clusters=cluster, init='k-means++', max_iter=5000, n_init=10, random_state=0)\n",
    "    pred_y = kmeans.fit_predict(transformed_sample)\n",
    "    #plt.scatter(transformed_sample[:,0], transformed_sample[:,1])\n",
    "    #plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
    "    #plt.show()\n",
    "    #scatter_plot(transformed_sample,kmeans.labels_)\n",
    "    '''\n",
    "    from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\n",
    "    colors = ['#4EACC5', '#FF9C34', '#4E9A06','#FF0000','#8800FF']\n",
    "    k_means_labels = pairwise_distances_argmin(transformed_sample, kmeans.cluster_centers_)\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    for k, col in zip(range(cluster), colors):\n",
    "        my_members = k_means_labels == k\n",
    "        cluster_center = kmeans.cluster_centers_[k]\n",
    "        ax.plot(transformed_sample[my_members, 0], transformed_sample[my_members, 1], 'w',\n",
    "                markerfacecolor=col, marker='.')\n",
    "        ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "                markeredgecolor='k', markersize=6)\n",
    "    experiment_name=experiment_name\n",
    "    ax.set_title(experiment_name)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    fig.savefig('Cohort/Models/Plots/'+experiment_name+'.png')'''\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(transformed_sample): \n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(transformed_sample)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(1, 11), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dbscan(transformed_sample,ellbow_method,cluster,df_name,num_scaler_name,cat_scaler_name,dim_red_method,experiment_name):\n",
    "    db = DBSCAN(eps=0.1, min_samples=5).fit(transformed_sample)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    scatter_plot(transformed_sample,labels) \n",
    "    \n",
    "    X=transformed_sample\n",
    "    \n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = X[class_member_mask & core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=14)\n",
    "\n",
    "        xy = X[class_member_mask & ~core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=6)\n",
    "\n",
    "    plt.title('DBSCAN' )\n",
    "    plt.show()\n",
    "    print(np.unique(labels))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hdbscan(transformed_sample,ellbow_method,cluster,df_name,num_scaler_name,cat_scaler_name,dim_red_method,experiment_name):\n",
    "    hdbscan_labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=500).fit_predict(transformed_sample)\n",
    "    clustered = (hdbscan_labels >= 0)\n",
    "    plt.scatter(transformed_sample[~clustered, 0],\n",
    "                transformed_sample[~clustered, 1],\n",
    "                c=(0.5, 0.5, 0.5),\n",
    "                s=0.1,\n",
    "                alpha=0.5)\n",
    "    plt.scatter(transformed_sample[clustered, 0],\n",
    "                transformed_sample[clustered, 1],\n",
    "                c=hdbscan_labels[clustered],\n",
    "                s=0.1,\n",
    "                cmap='Spectral');\n",
    "    \n",
    "    return hdbscan_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian(df,n_cluster):\n",
    "    gmm = GaussianMixture(n_components=n_cluster)\n",
    "    gmm.fit(df)\n",
    "    proba_lists = gmm.predict_proba(df)\n",
    "    #Plotting\n",
    "    colored_arrays = np.matrix(proba_lists)\n",
    "    colored_tuples = [tuple(i.tolist()[0]) for i in colored_arrays]\n",
    "    fig = plt.figure(1, figsize=(7,7))\n",
    "    ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)\n",
    "    ax.scatter(df[:, 0], df[:, 1], df[:, 2],\n",
    "              c=colored_tuples, edgecolor=\"k\", s=50)\n",
    "    ax.set_xlabel(\"Petal width\")\n",
    "    ax.set_ylabel(\"Sepal length\")\n",
    "    ax.set_zlabel(\"Petal length\")\n",
    "    plt.title(\"Gaussian Mixture Model\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierachical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hierachical(df_dim_red,ellbow_method,n_cluster,df_name,num_scaler_name,cat_scaler_name,dim_red_method,experiment_name):\n",
    "    Dendogram=False\n",
    "    #Dendogram: \n",
    "    if Dendogram==True:\n",
    "        try: \n",
    "        #load Dendogram image\n",
    "            plt.figure(figsize=(250, 7))\n",
    "            experiment_name=experiment_name\n",
    "            img=mpimg.imread('Cohort/Models/Plots/Dendograms/'+experiment_name+'.png')\n",
    "            print('Dendogram loaded')\n",
    "            imgplot = plt.imshow(img)\n",
    "            plt.show()\n",
    "        except:\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            experiment_name=df_name+'_'+num_scaler_name+'_'+cat_scaler_name+'_'+dim_red_method+'_'+str(dimension)+'_hierachical_'\n",
    "            plt.title(experiment_name)\n",
    "            dend = shc.dendrogram(shc.linkage(df_dim_red, method='ward'))  \n",
    "            plt.savefig('Cohort/Models/Plots/Dendograms/'+experiment_name+'.png')\n",
    "\n",
    "    # setting distance_threshold=0 ensures we compute the full tree.\n",
    "    model = AgglomerativeClustering(n_clusters=n_cluster, affinity='euclidean', linkage='ward',distance_threshold=None)\n",
    "\n",
    "    \n",
    "    model.fit_predict(df_dim_red)\n",
    "    scatter_plot(df_dim_red,model.labels_)\n",
    "   \n",
    "    \n",
    "    return model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_silhouette_Coefficient(labels,df):\n",
    "    m=metrics.silhouette_score(df, labels, metric='euclidean')\n",
    "    print('silhouette_score:',m)\n",
    "    return m\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calinski-Harabasz Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calinski_harabasz(labels,df):\n",
    "    m=metrics.calinski_harabasz_score(df, labels)\n",
    "    print('Calinski-Harabasz Index:',m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_davies_bouldin(labels,df):\n",
    "    m=davies_bouldin_score(df, labels)\n",
    "    print('Davies-Bouldin Index:',m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_num_column(df,col,kind): \n",
    "    total_patient=len(df)\n",
    "    df_mean=df[col].mean()\n",
    "    df_std= df[col].std()\n",
    "    df_median= df[col].median()\n",
    "    if kind=='Cluster':\n",
    "        row={'column_name':col,'col_type':'num','cat_total':'','cat_percentage':'','num_mean':df_mean,'num_std':df_std,'num_median':df_median,'total_patient':total_patient}\n",
    "    else: \n",
    "        row={'cat_total_all':'','cat_percentage_all':'','num_mean_all':df_mean,'num_std_all':df_std,'num_median_all':df_median,'total_patient_all':total_patient}\n",
    "    return row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_cat_column(df,col,kind): \n",
    "    total_patient=len(df)\n",
    "    cat_total=len(df.loc[df[col]==True])\n",
    "    cat_percentage=(cat_total/total_patient)\n",
    "    if kind=='Cluster':\n",
    "        row={'column_name':col,'col_type':'cat','cat_total':cat_total,'cat_percentage':cat_percentage,'num_mean':'','num_std':'','num_median':'','total_patient':total_patient}\n",
    "    else: \n",
    "        row={'cat_total_all':cat_total,'cat_percentage_all':cat_percentage,'num_mean_all':'','num_std_all':'','num_median_all':'','total_patient_all':total_patient}\n",
    "    return row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_gender_column(df,kind):\n",
    "    total_patient=len(df)\n",
    "    cat_total=len(df.loc[df['gender']=='Male'])\n",
    "    cat_percentage=(cat_total/total_patient)\n",
    "    if kind=='Cluster':\n",
    "        row1={'column_name':'gender_male','col_type':'cat','cat_total':cat_total,'cat_percentage':cat_percentage,'num_mean':'','num_std':'','num_median':'','total_patient':total_patient}\n",
    "    else: \n",
    "        row1={'cat_total_all':cat_total,'cat_percentage_all':cat_percentage,'num_mean_all':'','num_std_all':'','num_median_all':'','total_patient_all':total_patient}\n",
    "    cat_total=len(df.loc[df['gender']=='Female'])\n",
    "    cat_percentage=(cat_total/total_patient)\n",
    "    if kind=='Cluster':\n",
    "        row2={'column_name':'gender_female','col_type':'cat','cat_total':cat_total,'cat_percentage':cat_percentage,'num_mean':'','num_std':'','num_median':'','total_patient':total_patient}\n",
    "    else: \n",
    "        row2={'cat_total_all':cat_total,'cat_percentage_all':cat_percentage,'num_mean_all':'','num_std_all':'','num_median_all':'','total_patient_all':total_patient}\n",
    "    cat_total=len(df.loc[df['gender']=='Female'])\n",
    "    return row1, row2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_feature(ctransformer,df_cohort,n_cluster):\n",
    "    result_array=[]\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    num_columns=ctransformer.transformers[0][2]\n",
    "    cat_columns=ctransformer.transformers[1][2]\n",
    "    df_cohort[list(num_columns)] = min_max_scaler.fit_transform(df_cohort[list(num_columns)])\n",
    "    del cat_columns[:8]\n",
    "    col=['cat_total_all','cat_percentage_all','num_mean_all','num_std_all','num_median_all','total_patient_all']\n",
    "    result_all=pd.DataFrame(columns=col)\n",
    "    for col in num_columns: \n",
    "        row=analyse_num_column(df_cohort,col,'all')\n",
    "        result_all=result_all.append(row, ignore_index=True)\n",
    "    for col in cat_columns: \n",
    "        row=analyse_cat_column(df_cohort,col,'all')\n",
    "        result_all=result_all.append(row, ignore_index=True)\n",
    "    row=analyse_gender_column(df_cohort,'all')\n",
    "    result_all=result_all.append(row[0], ignore_index=True)\n",
    "    result_all=result_all.append(row[1], ignore_index=True)\n",
    "    for i in (range(n_cluster)):\n",
    "        col=['column_name','col_type','cat_total','cat_percentage','num_mean','num_std','num_median','total_patient']\n",
    "        result=pd.DataFrame(columns=col)\n",
    "        df=df_cohort.loc[df_cohort[dim_red_method]==i]\n",
    "        for col in num_columns: \n",
    "            row=analyse_num_column(df,col,'Cluster')\n",
    "            result=result.append(row, ignore_index=True)\n",
    "        for col in cat_columns: \n",
    "            row=analyse_cat_column(df,col,'Cluster')\n",
    "            result=result.append(row, ignore_index=True)\n",
    "        row=analyse_gender_column(df,'Cluster')\n",
    "        result=result.append(row[0], ignore_index=True)\n",
    "        result=result.append(row[1], ignore_index=True)\n",
    "        result=pd.concat([result,result_all],axis=1)\n",
    "\n",
    "        result_array.append(result)\n",
    "        \n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features(result_array,n_cluster,top_features):\n",
    "    for i in (range(n_cluster)):\n",
    "        test_num=result_array[i].loc[result_array[i]['col_type']=='num']\n",
    "        test_num['dif_median']=abs(test_num['num_median']-test_num['num_median_all'])\n",
    "        test_num=test_num.sort_values(by=['dif_median'],ascending=False)\n",
    "        print('Cluster '+str(i)+' num features \\n',test_num[['column_name' ,'num_median','num_median_all']].head(top_features))\n",
    "        test_cat=result_array[i].loc[result_array[i]['col_type']=='cat']\n",
    "        test_cat['dif_percentage']=abs(test_cat['cat_percentage']-test_cat['cat_percentage_all'])\n",
    "        test_cat=test_cat.sort_values(by=['dif_percentage'],ascending=False)\n",
    "        print('Cluster '+str(i)+' cat features \\n',test_cat[['column_name' ,'cat_percentage','cat_percentage_all']].head(top_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Anova "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_feature_importance_anova(df,ctransformer,dim_red_method,n_cluster,top_features):\n",
    "    df_temp=df\n",
    "    #replace cluster names \n",
    "    for cluster in (range(n_cluster)):\n",
    "        cluster_name='cluster_'+str(cluster)\n",
    "        df[dim_red_method].replace({cluster: cluster_name},inplace=True)\n",
    "    #normalize num columns\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    num_columns=ctransformer.transformers[0][2]\n",
    "    df_temp[list(num_columns)] = min_max_scaler.fit_transform(df_temp[list(num_columns)])\n",
    "    #iterate over num columns and calculate the p-Value: \n",
    "    col=['column name','F-Value','p-value','absolute_p','compared to other']\n",
    "    result_all=pd.DataFrame(columns=col) \n",
    "    result_anova=[]\n",
    "    for cluster in df_temp[dim_red_method].unique():\n",
    "        result_all=pd.DataFrame(columns=col)\n",
    "        df_temp['temp_cluster']=df_temp[dim_red_method]\n",
    "        df_temp.loc[df[dim_red_method] != cluster, \"temp_cluster\"] = \"other_cluster\"\n",
    "        for num_col in num_columns: \n",
    "            feature=num_col\n",
    "            result = df_temp.groupby('temp_cluster')[feature].apply(list)\n",
    "            #print(result)\n",
    "            feature_value_1=result[cluster]\n",
    "            #print(feature_value_1)\n",
    "            feature_value_2=result['other_cluster']\n",
    "            mean_1=mean(feature_value_1)\n",
    "            mean_2=mean(feature_value_2)\n",
    "            if mean_1 > mean_2: \n",
    "                compared='higher'\n",
    "            else:\n",
    "                compared='lower'\n",
    "            #print(len(result['cluster_3']))\n",
    "            #print(len(result['cluster_0']))\n",
    "            F, p = stats.f_oneway(*result)\n",
    "            p=format(p, '.300000000g')\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            row={'column name':(feature+'_'+cluster),'F-Value':F,'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        result_all=result_all.sort_values(by=['absolute_p'],ascending=False)\n",
    "        result_anova.append(result_all)\n",
    "    #result_all=result_all.drop_duplicates(subset='column name',keep='first', inplace=False)\n",
    "    #return result_all.head(top_features)\n",
    "    return result_anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pythonfordatascience.org/chi-square-test-of-independence-python/\n",
    "def cat_feature_importance(df,ctransformer,sup_colums,dim_red_method,n_cluster,top_features):\n",
    "    #replace cluster names \n",
    "    #establish two categories in all Categories \n",
    "    \n",
    "   \n",
    "    for cluster in (range(n_cluster)):\n",
    "        cluster_name='cluster_'+str(cluster)\n",
    "        df[dim_red_method].replace({cluster: cluster_name},inplace=True)\n",
    "    df=df.replace(True, 'Yes')\n",
    "    df=df.replace(False,'No')\n",
    "    df=df.fillna('No')\n",
    "    df=df.replace(1, 'Yes')\n",
    "    df=df.replace(0,'No')\n",
    "    df=df.fillna('No')\n",
    "    col=['column name','Pearson Chi-square','Cramers V','p-value','absolute_p','compared to other']\n",
    "    result_all=pd.DataFrame(columns=col)\n",
    "    result_chi=[]\n",
    "    for cluster in df[dim_red_method].unique():\n",
    "        result_all=pd.DataFrame(columns=col)\n",
    "        df['temp_cluster']=df[dim_red_method]\n",
    "        df.loc[df[dim_red_method] != cluster, \"temp_cluster\"] = \"other_cluster\"\n",
    "        #print(df[[dim_red_method,'temp_cluster']])     \n",
    "        cat_columns=ctransformer.transformers[1][2]\n",
    "        #iterate over cat columns and calculate the p-Value: \n",
    "        for cat_col in cat_columns: \n",
    "            feature=cat_col\n",
    "            crosstab, test_results, expected = rp.crosstab(df[feature], df['temp_cluster'],\n",
    "                                                   test= \"chi-square\",\n",
    "                                                   expected_freqs= True,\n",
    "                                                   prop= \"cell\")\n",
    "            p=format(test_results[\"results\"][1], '.300000000g')\n",
    "            #print(p)\n",
    "           # if test_results[\"results\"][1]!=0:\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            compared=''\n",
    "            if feature !='gender':\n",
    "                feature_count_1=len(df.loc[df['temp_cluster']==cluster])\n",
    "                feature_cluster=df.loc[df['temp_cluster']==cluster]\n",
    "                feature_percentage_1=(len(feature_cluster.loc[feature_cluster[feature]=='Yes'])/feature_count_1)\n",
    "                #print(feature_percentage_1)\n",
    "    \n",
    "                feature_count_2=len(df.loc[df['temp_cluster']=='other_cluster'])\n",
    "                feature_cluster_2=df.loc[df['temp_cluster']=='other_cluster']\n",
    "                feature_percentage_2=(len(feature_cluster_2.loc[feature_cluster_2[feature]=='Yes'])/feature_count_2)\n",
    "                #print(feature_percentage_2)\n",
    "                if feature_percentage_1 > feature_percentage_2: \n",
    "                    compared='higher'\n",
    "                else:\n",
    "                    compared='lower'\n",
    "            row={'column name':(feature+'_'+cluster),'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            #row={'column name':feature,'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        for cat_col in sup_colums: \n",
    "            #print('Calculaint Supervised features')\n",
    "            feature=cat_col\n",
    "            crosstab, test_results, expected = rp.crosstab(df[feature], df['temp_cluster'],\n",
    "                                                   test= \"chi-square\",\n",
    "                                                   expected_freqs= True,\n",
    "                                                   prop= \"cell\")\n",
    "            #print(crosstab)\n",
    "            p=format(test_results[\"results\"][1], '.300000000g')\n",
    "            #print(p)\n",
    "           # if test_results[\"results\"][1]!=0:\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            compare=''\n",
    "            if feature !='gender':\n",
    "                feature_count_1=len(df.loc[df['temp_cluster']==cluster])\n",
    "                feature_cluster=df.loc[df['temp_cluster']==cluster]\n",
    "                feature_percentage_1=(len(feature_cluster.loc[feature_cluster[feature]=='Yes'])/feature_count_1)\n",
    "               # print(feature_percentage_1)\n",
    "    \n",
    "                feature_count_2=len(df.loc[df['temp_cluster']=='other_cluster'])\n",
    "                feature_cluster_2=df.loc[df['temp_cluster']=='other_cluster']\n",
    "                feature_percentage_2=(len(feature_cluster_2.loc[feature_cluster_2[feature]=='Yes'])/feature_count_2)\n",
    "               # print(feature_percentage_2)\n",
    "                if feature_percentage_1 > feature_percentage_2: \n",
    "                    compared='higher'\n",
    "                else:\n",
    "                    compared='lower'\n",
    "            row={'column name':(feature+'_'+cluster),'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            #row={'column name':feature,'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        result_all=result_all.sort_values(by=['absolute_p'],ascending=False)\n",
    "        result_chi.append(result_all)\n",
    "    #result_all=result_all.drop_duplicates(subset='column name',keep='first', inplace=False)\n",
    "    #return result_all.head(top_features)\n",
    "    return result_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('Cohort/Models/ColumnTransformer/'+df_name+'_'+num_scaler_name+'_'+cat_scaler_name+'.pkl', 'rb') as f:\n",
    "#            ctransformer = pickle.load(f)\n",
    "#cat_columns=ctransformer.transformers[1][2]\n",
    "#cat_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_feature_importance_t_test(df,ctransformer,dim_red_method,n_cluster,top_features,inp_colums,merge_w_inpatient):\n",
    "    df_temp=df\n",
    "    #replace cluster names \n",
    "    for cluster in (range(n_cluster)):\n",
    "        cluster_name='cluster_'+str(cluster)\n",
    "        df[dim_red_method].replace({cluster: cluster_name},inplace=True)\n",
    "    #normalize num columns\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    num_columns=ctransformer.transformers[0][2]\n",
    "    if merge_w_inpatient==True:\n",
    "        inpatient=inp_colums[0]\n",
    "        num_columns.append(inpatient)\n",
    "    #print(num_columns)\n",
    "    df_temp[list(num_columns)] = min_max_scaler.fit_transform(df_temp[list(num_columns)])\n",
    "    #iterate over num columns and calculate the p-Value: \n",
    "    col=['column name','T-Statistics','p-value','absolute_p','compared to other']\n",
    "    result_all=pd.DataFrame(columns=col) \n",
    "    result_t_test=[]\n",
    "    for cluster in df_temp[dim_red_method].unique():\n",
    "        result_all=pd.DataFrame(columns=col)\n",
    "        df_temp['temp_cluster']=df_temp[dim_red_method]\n",
    "        df_temp.loc[df[dim_red_method] != cluster, \"temp_cluster\"] = \"other_cluster\"\n",
    "        for num_col in num_columns: \n",
    "            feature=num_col\n",
    "            feature_value_1=df_temp.loc[df_temp['temp_cluster']==cluster][feature].values\n",
    "            feature_value_2=df_temp.loc[df_temp['temp_cluster']==\"other_cluster\"][feature].values\n",
    "            statistics,p=stats.ttest_ind(feature_value_1, feature_value_2, equal_var = False)\n",
    "            mean_1=feature_value_1.mean()\n",
    "            mean_2=feature_value_2.mean()\n",
    "            if mean_1 > mean_2: \n",
    "                compared='higher'\n",
    "            else:\n",
    "                compared='lower' \n",
    "           # print(feature_value_1)\n",
    "           # print(feature_value_2)\n",
    "           # print(p)\n",
    "            p=format(p, '.300000000g')\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            row={'column name':(feature+'_'+cluster),'T-Statistics':statistics,'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        result_all=result_all.sort_values(by=['absolute_p'],ascending=False)\n",
    "        result_t_test.append(result_all)        \n",
    "    return result_t_test\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Cluster information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for statistics: \n",
    "def get_base_characteristic_value(df , characteristic , kind):    \n",
    "    if kind==\"mean\": \n",
    "        df_mean=df[characteristic].mean()\n",
    "        df_std= df[characteristic].std()\n",
    "        df_max= df[characteristic].max()\n",
    "        df_min= df[characteristic].min()\n",
    "        if characteristic == \"HF_Onset_age_in_days\":\n",
    "            base_characteristics_cohort=pd.DataFrame({'Variable': [characteristic+\"_mean\", characteristic+\"_std\", characteristic+\"_max\", characteristic+\"_min\"],\n",
    "                                                  'Value': [(df_mean/365), (df_std/365), (df_max/365), (df_min/365)],})\n",
    "        else:\n",
    "            base_characteristics_cohort=pd.DataFrame({'Variable': [characteristic+\"_mean\", characteristic+\"_std\", characteristic+\"_max\", characteristic+\"_min\"],\n",
    "                                                  'Value': [(df_mean), (df_std), (df_max), (df_min)],})\n",
    "        \n",
    "    if kind==\"count\":\n",
    "        base_characteristics_cohort=pd.DataFrame(columns=[\"Variable\",\"Value\"])\n",
    "        feature_value=df[characteristic].unique()\n",
    "        #print(feature_value)\n",
    "        for value in feature_value: \n",
    "            df_condition=df.loc[df[characteristic]==value]\n",
    "            df_percent= df_condition.shape[0]/df.shape[0]\n",
    "            #print(df_percent)\n",
    "            new_row1 = {'Variable': value+\"_total\",'Value': df_condition.shape[0]}\n",
    "            new_row2 = {'Variable': value+\"_relation\",'Value': df_percent}\n",
    "            base_characteristics_cohort=base_characteristics_cohort.append(new_row1, ignore_index=True)\n",
    "            base_characteristics_cohort=base_characteristics_cohort.append(new_row2, ignore_index=True)\n",
    "       # print(df_condition.shape[0], df_percent)\n",
    "    #print (base_characteristics_cohort)\n",
    "    return base_characteristics_cohort\n",
    "\n",
    "def get_base_characteristics(df, characteristics): \n",
    "    base_characteristics_cohort=pd.DataFrame(columns=[\"Variable\",\"Value\"])\n",
    "    for characteristic in characteristics:\n",
    "        intermediate_base_characteristics_cohort=get_base_characteristic_value(df,characteristic[0],characteristic[1])\n",
    "        base_characteristics_cohort=pd.concat([base_characteristics_cohort,intermediate_base_characteristics_cohort])\n",
    "    #print(base_characteristics_cohort)\n",
    "    return base_characteristics_cohort\n",
    "\n",
    "def get_cluster_information(df,dim_red_method,base_characteristics):\n",
    "    \n",
    "    baseline_characteristics=[]\n",
    "    for cluster in df[dim_red_method].unique(): \n",
    "        cluster_characteristics=[]\n",
    "        df_temp=df.loc[df[dim_red_method] == cluster]\n",
    "        df_base_characteristics=get_base_characteristics(df_temp, base_characteristics)\n",
    "        \n",
    "        cluster_characteristics.append(cluster)\n",
    "        cluster_characteristics.append(len(df_temp))\n",
    "        cluster_characteristics.append(df_base_characteristics)\n",
    "        baseline_characteristics.append(cluster_characteristics)\n",
    "    return baseline_characteristics\n",
    "def get_cluster_statistics(df,dim_red_method):\n",
    "    #load inpatient and EF dataframe \n",
    "    hospitalization = pq.read_table('Cohort/Feature_Extraction/days_in_hospital.parquet').to_pandas()\n",
    "    ef=pq.read_table('Cohort/Feature_Extraction/avg_EF.parquet').to_pandas()\n",
    "    #merge both to the df: \n",
    "    df_cohort=pd.merge(df, hospitalization, how='left', left_index=True, right_on='medical_record_number')\n",
    "    df_cohort=pd.merge(df_cohort, ef, how='left',left_index=True, right_on='medical_record_number')\n",
    "    #get average days in hospital per patient per cluster\n",
    "    base_characteristics=[\n",
    "        [ \"avg_ef\",\"mean\"],\n",
    "        [\"days_in_hospital\",\"mean\"],\n",
    "        [ \"HF_Onset_age_in_days\",\"mean\"],\n",
    "        [\"gender\",\"count\"]\n",
    "        ]\n",
    "    baseline_characteristics=get_cluster_information(df_cohort,dim_red_method,base_characteristics)\n",
    "    print (baseline_characteristics)\n",
    "    df_boxplt=df_cohort[[\"avg_ef\",dim_red_method]]\n",
    "    df_boxplt.boxplot(by=dim_red_method)\n",
    "    df_boxplt=df_cohort[[ \"days_in_hospital\",dim_red_method]]\n",
    "    df_boxplt.boxplot(by=dim_red_method)                   \n",
    "   \n",
    "\n",
    "    return baseline_characteristics\n",
    "        #print(str(cluster))\n",
    "        #print(len(df_temp))\n",
    "        \n",
    "        #print(df_temp_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subStringCluster(string):\n",
    "    a_string=string\n",
    "    split_string=a_string.split('_cluster_',1)\n",
    "    substring = split_string[0]\n",
    "    return substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overview_table(conv_df,features_tuples,features,dim_red_method): \n",
    "    feature_evaluation_df = pd.DataFrame()\n",
    "    feature_evaluation_df['features']=features\n",
    "    #print (feature_evaluation_df)\n",
    "    for cluster in conv_df[dim_red_method].unique(): \n",
    "        feature_evaluation_df[cluster]=0\n",
    "        cluster_df= conv_df.loc[conv_df[dim_red_method]==cluster]        \n",
    "        for features_tuple in features_tuples:\n",
    "            if features_tuple[1]=='categorical':\n",
    "                sum_feature=cluster_df[features_tuple[0]].sum()\n",
    "                percentage=sum_feature/len(cluster_df)\n",
    "                feature_evaluation_df.loc[feature_evaluation_df['features']==features_tuple[0],cluster]=percentage\n",
    "                \n",
    "                #print('categorical')\n",
    "            if features_tuple[1]=='numeric':\n",
    "                mean_feature=cluster_df[features_tuple[0]].mean()\n",
    "                median_feature=cluster_df[features_tuple[0]].median()\n",
    "                feature_evaluation_df.loc[feature_evaluation_df['features']==features_tuple[0],cluster]=str((str(mean_feature)+'/'+str(median_feature)))\n",
    "                #print('numeric') '''\n",
    "   # print(feature_evaluation_df)\n",
    "    return feature_evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopCluster(evaluation_pandas, n_topFeature, n_cluster ): \n",
    "    topFeatures=[]\n",
    "    for n in range(n_cluster):\n",
    "        #print(n)\n",
    "        features=[]\n",
    "         #categorical features\n",
    "        features=evaluation_pandas[2][n]['column name'].values\n",
    "        all_features = evaluation_pandas[2][n]\n",
    "        x=0\n",
    "        for i in range(n_topFeature):\n",
    "            feature=subStringCluster(features[x])\n",
    "            \n",
    "            if 'Procedure' in feature: \n",
    "               # print (feature)\n",
    "                #x=x+1\n",
    "                #print(subStringCluster(features[x]))\n",
    "                #topFeatures.append(subStringCluster(features[x]))\n",
    "                i=i-1\n",
    "            elif feature != 'gender' :\n",
    "                f=all_features.loc[all_features['column name']==features[x]]\n",
    "                p_value=f['p-value'].values\n",
    "                if p_value < 0.05 and p_value!=0.0 :\n",
    "                    topFeatures.append([subStringCluster(features[x]),'categorical'])\n",
    "                    #print(feature)\n",
    "                \n",
    "            else: \n",
    "                i=i-1\n",
    "            x=x+1\n",
    "        \n",
    "        #numeric\n",
    "        features=evaluation_pandas[1][n]['column name'].values\n",
    "        all_features = evaluation_pandas[1][n]\n",
    "        for i in range(n_topFeature):\n",
    "            f=all_features.loc[all_features['column name']==features[i]]\n",
    "            p_value=f['p-value'].values\n",
    "            if p_value < 0.05 and p_value!=0.0 :\n",
    "                topFeatures.append([subStringCluster(features[i]),'numeric'])\n",
    "    topFeatures_tuple=set(tuple(t)for t in topFeatures)\n",
    "    #print(topFeatures_tuple)\n",
    "    topFeatures=[t[0] for t in topFeatures_tuple]\n",
    "    #print(topFeatures)\n",
    "    #topFeatures=set(topFeatures)\n",
    "    #topFeatures=list(topFeatures)\n",
    "    #print(topFeatures)\n",
    "    return topFeatures_tuple, topFeatures\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/hpi-dhc/robotehr/blob/e3673aef701aa817c74d04170986f01fa191212a/robotehr/evaluation/risk_groups.py#L70-L100\n",
    "def plot_risk_groups(df, features,dim_red_method, friendly_names_converter=None, filename='', nrows=2, figsize=[12,3]):\n",
    "    #features=features[:2]\n",
    "    #ncols = int(len(features) / nrows)\n",
    "    ncols=1\n",
    "    #fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    fig, ax = plt.subplots(len(features),figsize=figsize)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    print(len(features))\n",
    "    for i in range(len(features)):\n",
    "        #row_index = int(i / ncols)\n",
    "        row_index=i\n",
    "        #col_index = i % int(len(features) / nrows)\n",
    "        col_index=0\n",
    "\n",
    "        #current_axis = ax[row_index][col_index] if nrows > 1 else ax[col_index]\n",
    "        current_axis = ax[row_index]\n",
    "        if df[features[i]].min() == 0 and df[features[i]].max() == 1:\n",
    "            current_axis.set_ylim(bottom=-0.5, top=1.5)\n",
    "        sns.violinplot(\n",
    "            x=dim_red_method,\n",
    "            y=features[i],\n",
    "            data=df,\n",
    "            palette=\"muted\",\n",
    "            ax=current_axis,\n",
    "            hue='gender'\n",
    "        )\n",
    "        if friendly_names_converter:\n",
    "            title = friendly_names_converter.get(features[i])\n",
    "        else:\n",
    "            title = features[i]\n",
    "        if len(title) > 50:\n",
    "            title = f'{title[:50]} ...'\n",
    "        current_axis.set_title(f'{title}', fontsize=20)\n",
    "        current_axis.set_xlabel('')\n",
    "        current_axis.set_ylabel('')\n",
    "    if filename:\n",
    "        fig.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature_names(feature_evaluation):\n",
    "    feature_look_up=pq.read_table('Cohort/feature_look_up.parquet').to_pandas()\n",
    "    feature_evaluation['human_readable']=''\n",
    "    for index,r in feature_evaluation.iterrows():\n",
    "        lookuplist=feature_look_up['original_feature_name'].to_list()\n",
    "        if r['features'] in lookuplist:\n",
    "            human_readable_row=feature_look_up.loc[feature_look_up['original_feature_name']==r['features']]\n",
    "            human_readable=human_readable_row['human_readable'].values\n",
    "            #print(human_readable)\n",
    "            feature_evaluation.loc[feature_evaluation['features']==r['features'],'human_readable']=human_readable[0]\n",
    "        else :\n",
    "            feature_evaluation.loc[feature_evaluation['features']==r['features'],'human_readable']=r['features']\n",
    "    return feature_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTopFeatures(df_path,df,merge_w_supervised,dim_red_method, evaluation_results,  n_cluster, n_topFeatures): \n",
    "    '''#convert the dataframe\n",
    "    df_origin=pq.read_table(df_path).to_pandas()\n",
    "    #print(df_origin['gender'])\n",
    "    df_origin[dim_red_method]=df[dim_red_method]\n",
    "    conv_df=df_origin\n",
    "    if merge_w_supervised==True:\n",
    "        df_supervised_merge= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drugFORMerge.parquet').to_pandas()\n",
    "        conv_df.index = conv_df.index.map(str)\n",
    "        df_supervised_merge.index = df_supervised_merge.index.map(str)\n",
    "        conv_df=pd.merge(conv_df, df_supervised_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "     '''\n",
    "    conv_df=pq.read_table(df_path).to_pandas()\n",
    "    if merge_w_supervised==True:\n",
    "            df_supervised_merge= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drugFORMerge_wLab.parquet').to_pandas()\n",
    "            conv_df.index = conv_df.index.map(str)\n",
    "            df_supervised_merge.index = df_supervised_merge.index.map(str)\n",
    "            conv_df=pd.merge(conv_df, df_supervised_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "        \n",
    "    conv_df=conv_df.replace(True, 1)\n",
    "    conv_df=conv_df.replace(False,0)\n",
    "    conv_df=conv_df.replace('yes', 1)\n",
    "    conv_df=conv_df.replace('no',0)\n",
    "    conv_df=conv_df.fillna(0)\n",
    "    conv_df[dim_red_method]=df[dim_red_method]\n",
    "    conv_df=conv_df.sort_values(by=[dim_red_method],ascending=True)\n",
    "    #get top featrues: \n",
    "    evaluation_pandas=evaluation_results\n",
    "    features_tuples,features=getTopCluster(evaluation_pandas, n_topFeatures, n_cluster)\n",
    "    #plot features \n",
    "    #print (cluster_name)\n",
    "   # fig_x=12*len(features)\n",
    "    fig_y=8*len(features)\n",
    "    plot_risk_groups(conv_df, features, dim_red_method,friendly_names_converter=None, filename='', nrows=10, figsize=[12,fig_y])\n",
    "    feature_evaluation_df=create_overview_table(conv_df,features_tuples,features,dim_red_method)\n",
    "    feature_evaluation_df=map_feature_names(feature_evaluation_df)\n",
    "    return feature_evaluation_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nice Scatter Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(df,labels):\n",
    "    sns.set(style='white', rc={'figure.figsize':(10,8)})\n",
    "    sns.color_palette(\"Set2\")\n",
    "    plt.scatter(df[:, 0], df[:, 1], c=labels, s=0.1, cmap='Accent');\n",
    "    plt.show()\n",
    "   # px.scatter(df[:, 0], df[:, 1], c=labels, s=0.1 ,color_continuous_scale=px.colors.sequential.Inferno);\n",
    "    #px.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_pipeline(df_path,df_name,age_filter,drop_gender,drop_age,tune_umap,umap_distance,umap_neighbors, check_pca,plotting,num_scaler_name,cat_scaler_name, dim_red_method,dimension,a_f_decoder,a_f_encoder,batchsize,epochs,optimizer,loss_function,cluster_method,ellbow_method,n_cluster,anova,chi,top_features,merge_w_supervised,merge_w_inpatient):\n",
    "    experiment_name=df_name+'_'+num_scaler_name+'_'+cat_scaler_name\n",
    "    if drop_gender==True:\n",
    "        experiment_name=experiment_name+'_woGender'\n",
    "        print(experiment_name)\n",
    "    if drop_age==True:\n",
    "        experiment_name=experiment_name+'_woAge'\n",
    "        print(experiment_name)\n",
    "    labels=[]\n",
    "    df_origin= pq.read_table(df_path).to_pandas()\n",
    "     #Age Filter:\n",
    "    if age_filter==True: \n",
    "        df_origin.loc[(df_origin['HF_Onset_age_in_days'] > 32850),'HF_Onset_age_in_days']=32850\n",
    "        #print(df_cohort)\n",
    "        \n",
    "    #general columns that should be not included in the clustering    \n",
    "    col_for_dropping=[\n",
    "        'religion',\n",
    "        'race',\n",
    "        'patient_ethnic_group',\n",
    "        'deceased_indicator',\n",
    "        'mother_account_number',\n",
    "        'address_zip',\n",
    "        'marital_status_code']\n",
    "    \n",
    "    #Exclude gender in Cluster analysis\n",
    "    if drop_gender==True: \n",
    "        col_for_dropping.append('gender')\n",
    "    #Exclude age from Cluster Analysis\n",
    "    if drop_age==True: \n",
    "        col_for_dropping.append('HF_Onset_age_in_days')\n",
    "    df_cohort=df_origin.drop(col_for_dropping,axis=1)\n",
    "    \n",
    "    #print(df_cohort)\n",
    "    #ColumnTransformer df,df_name,num_scaler_name,cat_scaler_name\n",
    "    a=apply_columnTransformer(df_cohort,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "    transformed_df= a[0]\n",
    "    ctransformer=a[1]\n",
    "    loss=0\n",
    "    n_layer=0\n",
    "    # test best PCA Dimension: \n",
    "    if check_pca==True:\n",
    "        pca = PCA().fit(transformed_df)\n",
    "        fig, ax = plt.subplots()\n",
    "        d_pca=np.cumsum(pca.explained_variance_ratio_)\n",
    "        #x=\"year\", y=\"passengers\"\n",
    "        #sns.set(style='white', rc={'figure.figsize':(12,10)})\n",
    "        g=sns.lineplot(data=d_pca,ax=ax)\n",
    "        g.set_xticklabels([0,25,50,75,100,125,150,250])\n",
    "        #g.set_yticklabels([0,0.25,0.50,0.75,1])\n",
    "        ax.set_xlim(0,300)\n",
    "        ax.set_ylim(0,1)\n",
    "        #ax.set_xticks(range(1,200))\n",
    "        plt.show()\n",
    "        #print(len(d))\n",
    "        #sns.lineplot(data=may_flights, x=\"year\", y=\"passengers\")\n",
    "        #sns.set(style='white', rc={'figure.figsize':(10,8)})\n",
    "        #fig,ax=plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "        #ax.set_xticks(range(1,250))\n",
    "        #plt.xlabel('number of components')\n",
    "        #plt.ylabel('cumulative explained variance');\n",
    "    #return True\n",
    "    #Dimension Reduction: \n",
    "    experiment_name=experiment_name+'_'+dim_red_method+'_'+str(dimension)\n",
    "    if tune_umap==True: \n",
    "        experiment_name=experiment_name+'_'+str(umap_distance)+'_'+str(umap_neighbors)\n",
    "    try: \n",
    "        if dim_red_method=='AE':\n",
    "            n_layer=3\n",
    "            load = np.load('Cohort/Models/DimReduction/'+df_name+'_'+num_scaler_name+'_'+cat_scaler_name+'_AE_'+str(dimension)+'_'+a_f_encoder+'_'+a_f_decoder+'_'+str(n_layer)+'_'+str(batchsize)+'_'+str(epochs)+'_'+optimizer+'_'+loss_function+'.npz')\n",
    "            df_dim_red=load['a']\n",
    "            print('df_dim_red loaded')\n",
    "        else:            \n",
    "            load = np.load('Cohort/Models/DimReduction/'+experiment_name+'.npz')\n",
    "            df_dim_red=load['a']\n",
    "            print('df_dim_red loaded!!!')\n",
    "    except:  \n",
    "        if dim_red_method==\"PPCA\": \n",
    "            df_dim_red=apply_ppca(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"TSNE\": \n",
    "            df_dim_red=apply_TSNE(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"ICA\": \n",
    "            df_dim_red=apply_ICA(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"SVD\": \n",
    "            df_dim_red=apply_svd(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"PCA\": \n",
    "            df_dim_red=apply_pca(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"IPCA\": \n",
    "            df_dim_red=apply_ipca(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"KPCA\": \n",
    "            df_dim_red=apply_kpca(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"LDA\": \n",
    "            df_dim_red=apply_lda(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"PPCA_TSNE\":\n",
    "            df_dim_red=apply_ppca(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "            df_dim_red=apply_TSNE(df_cohort,df_dim_red,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"PCA_TSNE\":\n",
    "            df_dim_red=apply_pca(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "            df_dim_red=apply_TSNE(df_cohort,df_dim_red,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"ICA_TSNE\":\n",
    "            df_dim_red=apply_ICA(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "            df_dim_red=apply_TSNE(df_cohort,df_dim_red,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"KPCA_TSNE\":\n",
    "            df_dim_red=apply_kpca(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "            df_dim_red=apply_TSNE(df_cohort,df_dim_red,dimension,df_name,num_scaler_name,cat_scaler_name,experiment_name)\n",
    "        if dim_red_method==\"UMAP\":\n",
    "            df_dim_red=apply_umap(df_cohort,transformed_df,dimension,umap_distance,umap_neighbors,df_name,num_scaler_name,cat_scaler_name,experiment_name,True)\n",
    "        if dim_red_method=='AE':\n",
    "            n_layer=3\n",
    "            df_dim_red,loss=apply_AE(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,a_f_decoder,a_f_encoder,n_layer,batchsize,epochs,optimizer,loss_function)\n",
    "        if dim_red_method=='AE_TSNE':\n",
    "            n_layer=3\n",
    "            df_dim_red,loss=apply_AE(df_cohort,transformed_df,dimension,df_name,num_scaler_name,cat_scaler_name,a_f_decoder,a_f_encoder,n_layer,batchsize,epochs,optimizer,loss_function)\n",
    "            df_dim_red=apply_TSNE(df_cohort,df_dim_red,dimension,df_name,num_scaler_name,cat_scaler_name)\n",
    "        if dim_red_method==\"\":\n",
    "            df_dim_red=transformed_df\n",
    "        if dim_red_method=='AE': \n",
    "            np.savez_compressed('Cohort/Models/DimReduction/'+df_name+'_'+num_scaler_name+'_'+cat_scaler_name+'_AE_'+str(dimension)+'_'+a_f_encoder+'_'+a_f_decoder+'_'+str(n_layer)+'_'+str(batchsize)+'_'+str(epochs)+'_'+optimizer+'_'+loss_function+'.npz',a=df_dim_red)\n",
    "        else:\n",
    "            np.savez_compressed('Cohort/Models/DimReduction/'+experiment_name+'.npz',a=df_dim_red)\n",
    "    \n",
    "    \n",
    "   #extend the experiment_name\n",
    "    experiment_name=experiment_name+'_'+cluster_method+'_'+str(n_cluster)\n",
    "    if cluster_method==\"kmeans\":\n",
    "        labels=apply_kmeans(df_dim_red,ellbow_method,n_cluster,df_name,num_scaler_name,cat_scaler_name,dim_red_method,experiment_name)\n",
    "    if cluster_method==\"gaussian\": \n",
    "        apply_gaussian(df_dim_red,4)\n",
    "    if cluster_method==\"hierarchical\":\n",
    "        labels=apply_hierachical(df_dim_red,ellbow_method,n_cluster,df_name,num_scaler_name,cat_scaler_name,dim_red_method,experiment_name)\n",
    "    if cluster_method==\"dbscan\":\n",
    "        labels=apply_dbscan(df_dim_red,ellbow_method,n_cluster,df_name,num_scaler_name,cat_scaler_name,dim_red_method,experiment_name)\n",
    "    if cluster_method==\"hdbscan\":\n",
    "        labels=apply_hdbscan(df_dim_red,ellbow_method,n_cluster,df_name,num_scaler_name,cat_scaler_name,dim_red_method,experiment_name)\n",
    "    \n",
    "    if plotting==True:      \n",
    "        #prepare data for plotting \n",
    "        df_dim_red_plot=apply_umap(df_cohort,df_dim_red,2,umap_distance,umap_neighbors,df_name,num_scaler_name,cat_scaler_name,experiment_name,False)        \n",
    "        #print first 2 dim of dimensionality reduced data:\n",
    "        scatter_plot(df_dim_red_plot,None)\n",
    "        scatter_plot(df_dim_red_plot,labels)\n",
    "    \n",
    "   # evaluation_results=[]\n",
    "    if len(labels)!=0: \n",
    "        evaluation_results=pq.read_table('Cohort/Models/Metrics_Results.parquet').to_pandas()\n",
    "        print(experiment_name)\n",
    "        if experiment_name in evaluation_results.values:\n",
    "            t=evaluation_results.loc[evaluation_results['Experiment Name'] == experiment_name]\n",
    "            print(t)\n",
    "        else :\n",
    "            print(labels)\n",
    "            silhouette_Coefficient= get_silhouette_Coefficient(labels,df_dim_red)\n",
    "            calinski_harabasz=get_calinski_harabasz(labels,df_dim_red)\n",
    "            davies_bouldin=get_davies_bouldin(labels,df_dim_red)\n",
    "            if dim_red_method!='PPCA'and dim_red_method!='PPCA_TSNE':\n",
    "                silhouette_Coefficient_original_Cohort= get_silhouette_Coefficient(labels,transformed_df)\n",
    "                calinski_harabasz_original_Cohort=get_calinski_harabasz(labels,transformed_df)\n",
    "                davies_bouldin_original_Cohort=get_davies_bouldin(labels,transformed_df)\n",
    "            else: \n",
    "                silhouette_Coefficient_original_Cohort=0\n",
    "                calinski_harabasz_original_Cohort=0\n",
    "                davies_bouldin_original_Cohort=0\n",
    "            evaluation_results=evaluation_results.append({'Experiment Name':experiment_name,'Dataset':df_name,'Numerical Scaler':num_scaler_name,'Categorical Scaler':cat_scaler_name,'Dimension Reduction Method':dim_red_method,'Number of Dimension':dimension,'Activation Function Decoder':a_f_decoder,'Activation Function Encoder':a_f_encoder,'Number of Layer':n_layer,'batchsize':str(batchsize),'epochs':str(epochs),'optimizer':optimizer,'loss function':loss_function,'validation loss':loss,'Cluster Method':cluster_method,'Number of Cluster':n_cluster,'silhouette_Coefficient':silhouette_Coefficient , 'calinski_harabasz':calinski_harabasz , 'davies_bouldin':davies_bouldin,'silhouette_Coefficient_original_Cohort':silhouette_Coefficient_original_Cohort , 'calinski_harabasz_original_Cohort':calinski_harabasz_original_Cohort , 'davies_bouldin_original_Cohort':davies_bouldin_original_Cohort} , ignore_index=True)\n",
    "            evaluation_results.to_parquet('Cohort/Models/Metrics_Results.parquet')\n",
    "        df_cohort[dim_red_method]=labels\n",
    "    \n",
    "        #result_array=analyse_feature(ctransformer,df_cohort,n_cluster)\n",
    "        #get_important_features(result_array,n_cluster,40)\n",
    "        #Test try to add the supervised features: \n",
    "        sup_colums=[]\n",
    "        inp_colums=[]\n",
    "        if merge_w_supervised==True:\n",
    "            df_supervised_merge= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drugFORMerge_wLab.parquet').to_pandas()\n",
    "            df_cohort.index = df_cohort.index.map(str)\n",
    "            df_supervised_merge.index = df_supervised_merge.index.map(str)\n",
    "            sup_colums=df_supervised_merge.columns\n",
    "            df_cohort=pd.merge(df_cohort, df_supervised_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "        if merge_w_inpatient==True:\n",
    "            df_inpatient_merge= pq.read_table('Cohort/Feature_Extraction/Supervised_ALL_HF/inpatient_events_merge_wLab.parquet').to_pandas()\n",
    "            df_cohort.index = df_cohort.index.map(str)\n",
    "            df_inpatient_merge.index = df_inpatient_merge.index.map(str)\n",
    "            inp_colums=df_inpatient_merge.columns\n",
    "            df_cohort=pd.merge(df_cohort, df_inpatient_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "            print(df_cohort)\n",
    "        #else: \n",
    "           # sup_colums=[]\n",
    "            #inp_colums=[] \n",
    "        df_cohort.to_parquet('Cohort/Models/Cluster/'+experiment_name+'.parquet')\n",
    "        #print(df_cohort)\n",
    "        evaluation_results=[]\n",
    "        df_origin[dim_red_method]=df_cohort[dim_red_method]\n",
    "        #added for without gender: NOT NEEEEDED\n",
    "        df_cohort['gender']=df_origin['gender']\n",
    "        df_cohort['HF_Onset_age_in_days']=df_origin['HF_Onset_age_in_days']\n",
    "        cluster_information=get_cluster_statistics(df_cohort,dim_red_method)\n",
    "        evaluation_results.append(cluster_information)\n",
    "        if anova==True: \n",
    "            top_numerical_features_anova=num_feature_importance_anova(df_cohort,ctransformer,dim_red_method,n_cluster,top_features)\n",
    "            print('Top Numerical features: \\n',top_numerical_features_anova)\n",
    "            evaluation_results.append(top_numerical_features_anova)\n",
    "        \n",
    "        if t_test==True: \n",
    "            top_numerical_features_t_test=num_feature_importance_t_test(df_cohort,ctransformer,dim_red_method,n_cluster,top_features,inp_colums,merge_w_inpatient)\n",
    "            print('Top Numerical features: \\n',top_numerical_features_t_test)\n",
    "            evaluation_results.append(top_numerical_features_t_test)   \n",
    "        if chi==True: \n",
    "            top_catigorical_features=cat_feature_importance(df_cohort,ctransformer,sup_colums,dim_red_method,n_cluster,top_features)\n",
    "            print('Top Categorical features: \\n',top_catigorical_features)\n",
    "            evaluation_results.append(top_catigorical_features)\n",
    "        for entry in evaluation_results: \n",
    "            print(entry)\n",
    "        np.savez_compressed('Cohort/Models/ClusterEvaluation/'+experiment_name+'_evaluation.npz', a=evaluation_results)\n",
    "    return df_cohort,evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipieline Configutations \n",
    "\n",
    "## Dataframe\n",
    "- df_path: Path to dataframe (String)\n",
    "- df_name: Name of dataframe (String)\n",
    "- age_filter: Age over 90 is fixed to 90 (Boolean)\n",
    "- drop_age: age will be not considered in the pipeline (Boolean)\n",
    "- drop_gender: gender will be not considered in the pipeline (Boolean)\n",
    "## Preprocessing\n",
    "- scaler: Encoder for Categorical Columns:\n",
    "    - num_scaler_name: \n",
    "        - StandardScaler\n",
    "        - MinMaxScaler\n",
    "    - cat_scaler_name:\n",
    "        - BinaryEncoder \n",
    "        - OneHotEncoder\n",
    "## Dimension Reduction Methods\n",
    "- dim_red_method:\n",
    "    - PPCA\n",
    "    - ICA\n",
    "    - PCA\n",
    "        - check_pca: Calculating the Variance represented by the diffreent numbers of dimensions(Boolean)\n",
    "    - KPCA\n",
    "    - TSNE\n",
    "    - SVD\n",
    "    - LDA\n",
    "    - PCA_TSNE\n",
    "    - ICA_TSNE\n",
    "    - AE\n",
    "         - a_f_decoder: Activation Function of the decoder \n",
    "         - a_f_encoder: Activation Function of the encoder \n",
    "         - batchsize\n",
    "         - epochs\n",
    "             -optimizer\n",
    "          - loss_function\n",
    "    - AE_TSNE\n",
    "    - UMAP\n",
    "        - tune_umap: different configurations are tried out (Boolean)\n",
    "        - umap_distace: Minimum Distance between the data points (Float)\n",
    "        - umap_neighbours: Number of Neighbours (Float)\n",
    "    \n",
    "- dimension: number of dimensions the dataset should be reduced to \n",
    "## Clustering\n",
    "- cluster_method: \n",
    "    - kmenas\n",
    "    - hierarchical (AgglomerativeClustering)\n",
    "- ellbow-method: True or false\n",
    "- n_cluster: number of cluster that should be applied to the dataset\n",
    "## Feature Evaluation\n",
    "- anova: apply anova test on numerical features\n",
    "- chi: apply chi test on categorical features\n",
    "- top_features: Number of top features that should be printed out \n",
    " \n",
    "## General\n",
    "- plotting: Plotting of Scatter plots (Boolean)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of CLuster Pipeline example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_path='Cohort/Feature_Extraction/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab.parquet'\n",
    "df_name='ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab'\n",
    "#df_path='Cohort/Feature_Extraction/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_small_cleaned.parquet'\n",
    "#df_name='ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_small_cleaned'\n",
    "#_wSupervised\n",
    "#df_path='Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drug.parquet'\n",
    "#df_name='ALL_HF_cohort_supervised_only_ever_diag_drug.parquet'\n",
    "age_filter=True\n",
    "drop_gender=True\n",
    "drop_age=True\n",
    "tune_umap=False\n",
    "umap_distance=0.1\n",
    "umap_neighbors=50\n",
    "check_pca=False\n",
    "plotting=True\n",
    "num_scaler_name=\"MinMaxScaler\"\n",
    "cat_scaler_name='BinaryEncoder'\n",
    "dim_red_method='UMAP'# 'ICA_TSNE'\n",
    "a_f_decoder=''\n",
    "a_f_encoder=''\n",
    "batchsize=''\n",
    "epochs=''\n",
    "optimizer=''\n",
    "loss_function=''\n",
    "cluster_method='kmeans'#'hierarchical'\n",
    "ellbow_method=False\n",
    "n_cluster=3\n",
    "dimension=70\n",
    "anova=False\n",
    "t_test=True\n",
    "chi=True\n",
    "top_features=40\n",
    "merge_w_supervised=True\n",
    "merge_w_inpatient=False\n",
    "df,evaluation_results=cluster_pipeline(df_path,df_name,age_filter,drop_gender,drop_age,tune_umap,umap_distance,umap_neighbors, check_pca,plotting,num_scaler_name,cat_scaler_name, dim_red_method,dimension,a_f_decoder,a_f_encoder,batchsize,epochs,optimizer,loss_function,cluster_method,ellbow_method,n_cluster,anova,chi,top_features,merge_w_supervised,merge_w_inpatient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Evaluation example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_evaluation_df=plotTopFeatures(df_path,df,merge_w_supervised,dim_red_method, evaluation_results,n_cluster ,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_evaluation_df.to_excel(\"Cohort/Models/Feature_evaluation_\"+dim_red_method+str(dimension)+cluster_method+\".xlsx\")\n",
    "#feature_evaluation_df.to_parquet(\"Cohort/Models/Feature_evaluation_\"+dim_red_method+str(dimension)+cluster_method+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTopFeatures(df,df_path,merge_w_supervised,dim_red_method, evaluation_results,n_cluster , 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Pipeleine Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path='Cohort/Feature_Extraction/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab.parquet'\n",
    "df_name='ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab'\n",
    "age_filter=True\n",
    "drop_gender=True\n",
    "drop_age=True\n",
    "tune_umap=True \n",
    "arr_umap_distance=[0.5]\n",
    "arr_umap_neighbors=[15,50,100]\n",
    "check_pca=False\n",
    "plotting=False\n",
    "num_scaler_name=\"MinMaxScaler\"\n",
    "cat_scaler_name='BinaryEncoder'\n",
    "a_f_decoder=''\n",
    "a_f_encoder=''\n",
    "batchsize=''\n",
    "epochs=''\n",
    "optimizer=''\n",
    "loss_function=''\n",
    "ellbow_method=False\n",
    "#n_cluster=3\n",
    "anova=False\n",
    "t_test=False\n",
    "chi=False\n",
    "top_features=40\n",
    "merge_w_supervised=False\n",
    "merge_w_inpatient=False\n",
    "dim_red_method='UMAP'\n",
    "arr_cluster_method=['kmeans','hierarchical']\n",
    "arr_dimension=[50,60,70,80,90]\n",
    "arr_n_cluster=[3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run experiments for AE in a loop: \n",
    "df_path='Cohort/Feature_Extraction/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab.parquet'\n",
    "df_name='ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab'\n",
    "age_filter=True\n",
    "drop_gender=True\n",
    "drop_age=True\n",
    "tune_umap=True\n",
    "umap_distance=0.1\n",
    "umap_neighbors=50\n",
    "check_pca=False\n",
    "plotting=False\n",
    "num_scaler_name=\"MinMaxScaler\"\n",
    "cat_scaler_name='BinaryEncoder'\n",
    "a_f_decoder='tanh'\n",
    "a_f_encoder='tanh'\n",
    "batchsize=1000\n",
    "epochs=100\n",
    "optimizer='adam'\n",
    "loss_function='mse'\n",
    "ellbow_method=False\n",
    "#n_cluster=3\n",
    "anova=False\n",
    "t_test=False\n",
    "chi=False\n",
    "top_features=40\n",
    "merge_w_supervised=False\n",
    "merge_w_inpatient=False\n",
    "arr_dim_red_method=['AE']\n",
    "arr_cluster_method=['kmeans','hierarchical']\n",
    "arr_dimension=[50,60,70,80,90]\n",
    "arr_n_cluster=[3,4,5]\n",
    "x=0\n",
    "for r in arr_dim_red_method: \n",
    "    dim_red_method=r\n",
    "    for c in arr_cluster_method :\n",
    "        cluster_method=c\n",
    "        for d in arr_dimension:\n",
    "            dimension=d\n",
    "            for n in arr_n_cluster:\n",
    "                n_cluster=n\n",
    "                x=x+1\n",
    "                df,evaluation_results=cluster_pipeline(df_path,df_name,age_filter,drop_gender,drop_age,tune_umap,umap_distance,umap_neighbors, check_pca,plotting,num_scaler_name,cat_scaler_name, dim_red_method,dimension,a_f_decoder,a_f_encoder,batchsize,epochs,optimizer,loss_function,cluster_method,ellbow_method,n_cluster,anova,chi,top_features,merge_w_supervised,merge_w_inpatient)\n",
    "                print(x)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run experiments in a loop for UMAP: \n",
    "df_path='Cohort/Feature_Extraction/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab.parquet'\n",
    "df_name='ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab'\n",
    "age_filter=True\n",
    "drop_gender=True\n",
    "drop_age=True\n",
    "tune_umap=True \n",
    "arr_umap_distance=[0.5]\n",
    "arr_umap_neighbors=[15,50,100]\n",
    "check_pca=False\n",
    "plotting=False\n",
    "num_scaler_name=\"MinMaxScaler\"\n",
    "cat_scaler_name='BinaryEncoder'\n",
    "a_f_decoder=''\n",
    "a_f_encoder=''\n",
    "batchsize=''\n",
    "epochs=''\n",
    "optimizer=''\n",
    "loss_function=''\n",
    "ellbow_method=False\n",
    "#n_cluster=3\n",
    "anova=False\n",
    "t_test=False\n",
    "chi=False\n",
    "top_features=40\n",
    "merge_w_supervised=False\n",
    "merge_w_inpatient=False\n",
    "dim_red_method='UMAP'\n",
    "arr_cluster_method=['kmeans','hierarchical']\n",
    "arr_dimension=[50,60,70,80,90]\n",
    "arr_n_cluster=[3,4,5]\n",
    "x=0\n",
    "for dist in arr_umap_distance: \n",
    "    umap_distance=dist\n",
    "    for neighbors in arr_umap_neighbors: \n",
    "        umap_neighbors=neighbors\n",
    "        for c in arr_cluster_method :\n",
    "            cluster_method=c\n",
    "            for d in arr_dimension:\n",
    "                dimension=d\n",
    "                for n in arr_n_cluster:\n",
    "                    n_cluster=n\n",
    "                    x=x+1\n",
    "                    df,evaluation_results=cluster_pipeline(df_path,df_name,age_filter,drop_gender,drop_age,tune_umap,umap_distance,umap_neighbors,check_pca,plotting,num_scaler_name,cat_scaler_name,dim_red_method,dimension,a_f_decoder,a_f_encoder,batchsize,epochs,optimizer,loss_function,cluster_method,ellbow_method,n_cluster,anova,chi,top_features,merge_w_supervised,merge_w_inpatient)\n",
    "                    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run experiments in a loop: \n",
    "df_path='Cohort/Feature_Extraction/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab.parquet'\n",
    "df_name='ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab'\n",
    "age_filter=True\n",
    "drop_gender=True\n",
    "drop_age=True\n",
    "tune_umap=True\n",
    "umap_distance=0.1\n",
    "umap_neighbors=50\n",
    "check_pca=False\n",
    "plotting=False\n",
    "num_scaler_name=\"MinMaxScaler\"\n",
    "cat_scaler_name='BinaryEncoder'\n",
    "a_f_decoder=''\n",
    "a_f_encoder=''\n",
    "batchsize=''\n",
    "epochs=''\n",
    "optimizer=''\n",
    "loss_function=''\n",
    "ellbow_method=False\n",
    "n_cluster=3\n",
    "anova=False\n",
    "t_test=False\n",
    "chi=False\n",
    "top_features=40\n",
    "merge_w_supervised=False\n",
    "merge_w_inpatient=False\n",
    "arr_dim_red_method=['PCA','ICA','SVD','UMAP']\n",
    "arr_cluster_method=['kmeans','hierarchical']\n",
    "arr_dimension=[50,60,70,80,90]\n",
    "arr_n_cluster=[3,4,5]\n",
    "x=0\n",
    "for r in arr_dim_red_method: \n",
    "    dim_red_method=r\n",
    "    for c in arr_cluster_method :\n",
    "        cluster_method=c\n",
    "        for d in arr_dimension:\n",
    "            dimension=d\n",
    "            for n in arr_n_cluster:\n",
    "                n_cluster=n\n",
    "                x=x+1\n",
    "                df,evaluation_results=cluster_pipeline(df_path,df_name,age_filter,drop_gender,drop_age,umap_distance,umap_neighbors,check_pca,plotting,num_scaler_name,cat_scaler_name,dim_red_method,dimension,a_f_decoder,a_f_encoder,batchsize,epochs,optimizer,loss_function,cluster_method,ellbow_method,n_cluster,anova,chi,top_features,merge_w_supervised,merge_w_inpatient)\n",
    "                print(x)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting of the result overvie table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "evaluation_metrics=pq.read_table('Cohort/Models/Metrics_Results.parquet').to_pandas()\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "evaluation_metrics=evaluation_metrics.sort_values([ \"calinski_harabasz\",\"silhouette_Coefficient\", \"davies_bouldin\"], ascending = (False, False,True))\n",
    "s = evaluation_metrics.style.background_gradient(cmap=cm)\n",
    "s\n",
    "#s.to_excel(\"Cohort/Models/10_26_Evaluation.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new Result Array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['Experiment Name','Dataset','Numerical Scaler','Categorical Scaler','Dimension Reduction Method','Number of Dimension','Activation Function Decoder','Activation Function Encoder','Number of Layer','batchsize','epochs','optimizer','loss function','validation loss','Cluster Method','Number of Cluster','silhouette_Coefficient' , 'calinski_harabasz' , 'davies_bouldin','silhouette_Coefficient_original_Cohort' , 'calinski_harabasz_original_Cohort' , 'davies_bouldin_original_Cohort']\n",
    "result=pd.DataFrame(columns=col)\n",
    "result.to_parquet('Cohort/Models/Metrics_Results.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results=pq.read_table('Cohort/Models/Metrics_Results.parquet').to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "evaluation_results=evaluation_results.sort_values([\"silhouette_Coefficient\", \"calinski_harabasz\", \"davies_bouldin\"], ascending = (False, False,True))\n",
    "s = evaluation_results.style.background_gradient(cmap=cm)\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
