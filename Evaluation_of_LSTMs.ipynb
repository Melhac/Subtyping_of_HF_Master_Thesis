{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm autoencoder to recreate a timeseries\n",
    "#gridsearch: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "#https://machinelearningmastery.com/how-to-grid-search-deep-learning-models-for-time-series-forecasting/\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "import pyarrow.parquet as pq\n",
    "from gensim.models import Word2Vec\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn import preprocessing\n",
    "##### REQUIRES THE DATAFRAME FOLDER TO BE NAMED 'Cohorts', WHICH INCLUDES ALL PRECOMPUTED DATAFRAMES #####\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from functools import reduce\n",
    "from ppca import PPCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "import pickle\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import scipy.stats as stats\n",
    "import researchpy as rp\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from keras.layers import Input, Dense \n",
    "from keras.models import Model, Sequential \n",
    "from keras import regularizers \n",
    "import umap\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(per_day,embedding_method,embedding_size):\n",
    "    if per_day: \n",
    "        df_name='Timeseries_per_patient_per_day_LSTM_Data_embedded_'\n",
    "        data_structure='per_patient'\n",
    "    else : \n",
    "        df_name='Timeseries_per_patient_LSTM_Data_embedded_'\n",
    "        data_structure='per_patient'\n",
    "    if embedding_method=='cbow':\n",
    "        df_name=df_name+'cbow_dim{}_win5_mc0'.format(embedding_size)\n",
    "    if embedding_method=='skipgram':\n",
    "        df_name=df_name+'skipgram_dim{}_win5_mc0'.format(embedding_size)\n",
    "    print(df_name)\n",
    "    with open(\"Cohort/Time_Series/\"+df_name+'.txt', \"rb\") as fp:   # Unpickling\n",
    "        data = pickle.load(fp)\n",
    "    data_sample= data[:300]\n",
    "    return data,data_sample, df_name, data_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lstm_representation(data_structure,timesteps,n_features,layer_size_1,layer_size_2,activation_func,optimizer_func,loss_func,n_epochs,n_batch_size,df_name,data):\n",
    "    #create experiment Name to Load model \n",
    "    #experiment_name=df_name+'_'+data_structure+str(layer_size_1)+'_'+str(layer_size_2)+'_'+activation_func+'_'+optimizer_func+'_'+loss_func+'_'+str(n_epochs)+'_'+str(n_batch_size)\n",
    "    experiment_name=df_name+'_'+data_structure+str(layer_size_1)+'_'+str(layer_size_2)+'_'+activation_func+'_'+optimizer_func+'_'+loss_func+'_'+str(n_epochs)+'_'+str(n_batch_size)\n",
    "\n",
    "\n",
    "    \n",
    "    model = keras.models.load_model('Cohort/LSTM_Models/'+experiment_name)\n",
    "    hidden_representation = Sequential() \n",
    "    hidden_representation.add(model.layers[0]) \n",
    "    hidden_representation.add(model.layers[1])\n",
    "    dim_red_data=hidden_representation.predict(data)\n",
    "    print('data transformed')\n",
    "    return dim_red_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeans(dim_red_data,cluster,ellbow_method):\n",
    "    if ellbow_method==True:\n",
    "        elbow_method(dim_red_data)\n",
    "    kmeans = KMeans(n_clusters=cluster, init='k-means++', max_iter=5000, n_init=10, random_state=0)\n",
    "    pred_y = kmeans.fit_predict(dim_red_data)\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(transformed_sample): \n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(transformed_sample)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(1, 11), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hierachical(dim_red_data,n_cluster):\n",
    "    Dendogram=False\n",
    "    #Dendogram: \n",
    "    if Dendogram==True:\n",
    "        try: \n",
    "        #load Dendogram image\n",
    "            plt.figure(figsize=(250, 7))\n",
    "            experiment_name=experiment_name\n",
    "            img=mpimg.imread('Cohort/LSTM/Plots/Dendograms/'+experiment_name+'.png')\n",
    "            print('Dendogram loaded')\n",
    "            imgplot = plt.imshow(img)\n",
    "            plt.show()\n",
    "        except:\n",
    "            plt.figure(figsize=(10, 7))\n",
    "            experiment_name=df_name+'_'+num_scaler_name+'_'+cat_scaler_name+'_'+dim_red_method+'_'+str(dimension)+'_hierachical_'\n",
    "            plt.title(experiment_name)\n",
    "            dend = shc.dendrogram(shc.linkage(df_dim_red, method='ward'))  \n",
    "            plt.savefig('Cohort/LSTM/Plots/Dendograms/'+experiment_name+'.png')\n",
    "\n",
    "    # setting distance_threshold=0 ensures we compute the full tree.\n",
    "    model = AgglomerativeClustering(n_clusters=n_cluster, affinity='euclidean', linkage='ward',distance_threshold=None)\n",
    "\n",
    "    \n",
    "    model.fit_predict(dim_red_data)\n",
    "    #scatter_plot(dim_red_data,model.labels_)\n",
    "   \n",
    "    \n",
    "    return model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap(dim_red_data,dimension):\n",
    "    clusterable_embedding = umap.UMAP(\n",
    "        n_neighbors=50,\n",
    "        min_dist=0.1,\n",
    "        n_components=dimension,\n",
    "        random_state=42,\n",
    "    )\n",
    "    X_transformed=clusterable_embedding.fit_transform(dim_red_data)\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(df,labels):\n",
    "    sns.set(style='white', rc={'figure.figsize':(10,8)})\n",
    "    sns.color_palette(\"Set2\")\n",
    "    plt.scatter(df[:, 0], df[:, 1], c=labels, s=0.1, cmap='Accent');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_silhouette_Coefficient(labels,df):\n",
    "    m=metrics.silhouette_score(df, labels, metric='euclidean')\n",
    "    print('silhouette_score:',m)\n",
    "    return m\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calinski-Harabasz Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calinski_harabasz(labels,df):\n",
    "    m=metrics.calinski_harabasz_score(df, labels)\n",
    "    print('Calinski-Harabasz Index:',m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_davies_bouldin(labels,df):\n",
    "    m=davies_bouldin_score(df, labels)\n",
    "    print('Davies-Bouldin Index:',m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster informations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for statistics: \n",
    "def get_base_characteristic_value(df , characteristic , kind):    \n",
    "    if kind==\"mean\": \n",
    "        df_mean=df[characteristic].mean()\n",
    "        df_std= df[characteristic].std()\n",
    "        df_max= df[characteristic].max()\n",
    "        df_min= df[characteristic].min()\n",
    "        if characteristic == \"HF_Onset_age_in_days\":\n",
    "            base_characteristics_cohort=pd.DataFrame({'Variable': [characteristic+\"_mean\", characteristic+\"_std\", characteristic+\"_max\", characteristic+\"_min\"],\n",
    "                                                  'Value': [(df_mean/365), (df_std/365), (df_max/365), (df_min/365)],})\n",
    "        else:\n",
    "            base_characteristics_cohort=pd.DataFrame({'Variable': [characteristic+\"_mean\", characteristic+\"_std\", characteristic+\"_max\", characteristic+\"_min\"],\n",
    "                                                  'Value': [(df_mean), (df_std), (df_max), (df_min)],})\n",
    "        \n",
    "    if kind==\"count\":\n",
    "        base_characteristics_cohort=pd.DataFrame(columns=[\"Variable\",\"Value\"])\n",
    "        feature_value=df[characteristic].unique()\n",
    "        #print(feature_value)\n",
    "        for value in feature_value: \n",
    "            df_condition=df.loc[df[characteristic]==value]\n",
    "            df_percent= df_condition.shape[0]/df.shape[0]\n",
    "            #print(df_percent)\n",
    "            new_row1 = {'Variable': value+\"_total\",'Value': df_condition.shape[0]}\n",
    "            new_row2 = {'Variable': value+\"_relation\",'Value': df_percent}\n",
    "            base_characteristics_cohort=base_characteristics_cohort.append(new_row1, ignore_index=True)\n",
    "            base_characteristics_cohort=base_characteristics_cohort.append(new_row2, ignore_index=True)\n",
    "       # print(df_condition.shape[0], df_percent)\n",
    "    #print (base_characteristics_cohort)\n",
    "    return base_characteristics_cohort\n",
    "\n",
    "def get_base_characteristics(df, characteristics): \n",
    "    base_characteristics_cohort=pd.DataFrame(columns=[\"Variable\",\"Value\"])\n",
    "    for characteristic in characteristics:\n",
    "        intermediate_base_characteristics_cohort=get_base_characteristic_value(df,characteristic[0],characteristic[1])\n",
    "        base_characteristics_cohort=pd.concat([base_characteristics_cohort,intermediate_base_characteristics_cohort])\n",
    "    #print(base_characteristics_cohort)\n",
    "    return base_characteristics_cohort\n",
    "\n",
    "def get_cluster_information(df,dim_red_method,base_characteristics):\n",
    "    \n",
    "    baseline_characteristics=[]\n",
    "    for cluster in df[dim_red_method].unique(): \n",
    "        cluster_characteristics=[]\n",
    "        df_temp=df.loc[df[dim_red_method] == cluster]\n",
    "        df_base_characteristics=get_base_characteristics(df_temp, base_characteristics)\n",
    "        \n",
    "        cluster_characteristics.append(cluster)\n",
    "        cluster_characteristics.append(len(df_temp))\n",
    "        cluster_characteristics.append(df_base_characteristics)\n",
    "        baseline_characteristics.append(cluster_characteristics)\n",
    "    return baseline_characteristics\n",
    "def get_cluster_statistics(df,dim_red_method):\n",
    "    #load inpatient and EF dataframe \n",
    "    hospitalization = pq.read_table('Cohort/Feature_Extraction/days_in_hospital.parquet').to_pandas()\n",
    "    ef=pq.read_table('Cohort/Feature_Extraction/avg_EF.parquet').to_pandas()\n",
    "    #merge both to the df: \n",
    "    df_cohort=pd.merge(df, hospitalization, how='left', left_index=True, right_on='medical_record_number')\n",
    "    df_cohort=pd.merge(df_cohort, ef, how='left',left_index=True, right_on='medical_record_number')\n",
    "    #get average days in hospital per patient per cluster\n",
    "    base_characteristics=[\n",
    "        [ \"avg_ef\",\"mean\"],\n",
    "        [\"days_in_hospital\",\"mean\"],\n",
    "        [ \"HF_Onset_age_in_days\",\"mean\"],\n",
    "        [\"gender\",\"count\"]\n",
    "        ]\n",
    "    baseline_characteristics=get_cluster_information(df_cohort,dim_red_method,base_characteristics)\n",
    "    print (baseline_characteristics)\n",
    "    #calculate t-statistics: \n",
    "    cl_0=df_cohort.loc[df_cohort[dim_red_method]=='Cluster_0']\n",
    "    cl_1=df_cohort.loc[df_cohort[dim_red_method]=='Cluster_1']\n",
    "    cl_2=df_cohort.loc[df_cohort[dim_red_method]=='Cluster_2']\n",
    "    t_test_avg_ef=ttest_ind(cl_0['avg_ef'], cl_1['avg_ef'],cl_2['avg_ef'])\n",
    "    print(t_test_avg_ef)\n",
    "    return baseline_characteristics\n",
    "        #print(str(cluster))\n",
    "        #print(len(df_temp))\n",
    "        \n",
    "        #print(df_temp_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for statistics: \n",
    "def get_base_characteristic_value(df , characteristic , kind):    \n",
    "    if kind==\"mean\": \n",
    "        df_mean=df[characteristic].mean()\n",
    "        df_std= df[characteristic].std()\n",
    "        df_max= df[characteristic].max()\n",
    "        df_min= df[characteristic].min()\n",
    "        if characteristic == \"HF_Onset_age_in_days\":\n",
    "            base_characteristics_cohort=pd.DataFrame({'Variable': [characteristic+\"_mean\", characteristic+\"_std\", characteristic+\"_max\", characteristic+\"_min\"],\n",
    "                                                  'Value': [(df_mean/365), (df_std/365), (df_max/365), (df_min/365)],})\n",
    "        else:\n",
    "            base_characteristics_cohort=pd.DataFrame({'Variable': [characteristic+\"_mean\", characteristic+\"_std\", characteristic+\"_max\", characteristic+\"_min\"],\n",
    "                                                  'Value': [(df_mean), (df_std), (df_max), (df_min)],})\n",
    "        \n",
    "    if kind==\"count\":\n",
    "        base_characteristics_cohort=pd.DataFrame(columns=[\"Variable\",\"Value\"])\n",
    "        feature_value=df[characteristic].unique()\n",
    "        #print(feature_value)\n",
    "        for value in feature_value: \n",
    "            df_condition=df.loc[df[characteristic]==value]\n",
    "            df_percent= df_condition.shape[0]/df.shape[0]\n",
    "            #print(df_percent)\n",
    "            new_row1 = {'Variable': value+\"_total\",'Value': df_condition.shape[0]}\n",
    "            new_row2 = {'Variable': value+\"_relation\",'Value': df_percent}\n",
    "            base_characteristics_cohort=base_characteristics_cohort.append(new_row1, ignore_index=True)\n",
    "            base_characteristics_cohort=base_characteristics_cohort.append(new_row2, ignore_index=True)\n",
    "       # print(df_condition.shape[0], df_percent)\n",
    "    #print (base_characteristics_cohort)\n",
    "    return base_characteristics_cohort\n",
    "\n",
    "def get_base_characteristics(df, characteristics): \n",
    "    base_characteristics_cohort=pd.DataFrame(columns=[\"Variable\",\"Value\"])\n",
    "    for characteristic in characteristics:\n",
    "        intermediate_base_characteristics_cohort=get_base_characteristic_value(df,characteristic[0],characteristic[1])\n",
    "        base_characteristics_cohort=pd.concat([base_characteristics_cohort,intermediate_base_characteristics_cohort])\n",
    "    #print(base_characteristics_cohort)\n",
    "    return base_characteristics_cohort\n",
    "\n",
    "def get_cluster_information(df,dim_red_method,base_characteristics):\n",
    "    \n",
    "    baseline_characteristics=[]\n",
    "    for cluster in df[dim_red_method].unique(): \n",
    "        cluster_characteristics=[]\n",
    "        df_temp=df.loc[df[dim_red_method] == cluster]\n",
    "        df_base_characteristics=get_base_characteristics(df_temp, base_characteristics)\n",
    "        \n",
    "        cluster_characteristics.append(cluster)\n",
    "        cluster_characteristics.append(len(df_temp))\n",
    "        cluster_characteristics.append(df_base_characteristics)\n",
    "        baseline_characteristics.append(cluster_characteristics)\n",
    "    return baseline_characteristics\n",
    "def get_cluster_statistics(df,dim_red_method,boxplot):\n",
    "    #load inpatient and EF dataframe \n",
    "    hospitalization = pq.read_table('Cohort/Feature_Extraction/days_in_hospital.parquet').to_pandas()\n",
    "    ef=pq.read_table('Cohort/Feature_Extraction/avg_EF.parquet').to_pandas()\n",
    "    #merge both to the df: \n",
    "    df_cohort=pd.merge(df, hospitalization, how='left', left_index=True, right_on='medical_record_number')\n",
    "    df_cohort=pd.merge(df_cohort, ef, how='left',left_index=True, right_on='medical_record_number')\n",
    "    #get average days in hospital per patient per cluster\n",
    "    base_characteristics=[\n",
    "        [ \"avg_ef\",\"mean\"],\n",
    "        [\"days_in_hospital\",\"mean\"],\n",
    "        [ \"HF_Onset_age_in_days\",\"mean\"],\n",
    "        [\"gender\",\"count\"]\n",
    "        ]\n",
    "    baseline_characteristics=get_cluster_information(df_cohort,dim_red_method,base_characteristics)\n",
    "    print (baseline_characteristics)\n",
    "    if boxplot == True: \n",
    "        df_boxplt=df_cohort[[\"avg_ef\",dim_red_method]]\n",
    "        df_boxplt.boxplot(by=dim_red_method)\n",
    "        df_boxplt=df_cohort[[ \"days_in_hospital\",dim_red_method]]\n",
    "        df_boxplt.boxplot(by=dim_red_method)                   \n",
    "\n",
    "\n",
    "    return baseline_characteristics\n",
    "        #print(str(cluster))\n",
    "        #print(len(df_temp))\n",
    "        \n",
    "        #print(df_temp_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHI test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pythonfordatascience.org/chi-square-test-of-independence-python/\n",
    "def cat_feature_importance(df,ctransformer,sup_colums,dim_red_method,n_cluster):\n",
    "    \n",
    "   \n",
    "    \n",
    "    #replace cluster names \n",
    "    #establish two categories in all Categories \n",
    "    \n",
    "   \n",
    "    for cluster in (range(n_cluster)):\n",
    "        cluster_name='cluster_'+str(cluster)\n",
    "        df[dim_red_method].replace({cluster: cluster_name},inplace=True)\n",
    "    df=df.replace(True, 'Yes')\n",
    "    df=df.replace(False,'No')\n",
    "    df=df.fillna('No')\n",
    "    df=df.replace(1, 'Yes')\n",
    "    df=df.replace(0,'No')\n",
    "    df=df.fillna('No')\n",
    "    col=['column name','Pearson Chi-square','Cramers V','p-value','absolute_p','compared to other']\n",
    "    result_all=pd.DataFrame(columns=col)\n",
    "    result_chi=[]\n",
    "    for cluster in df[dim_red_method].unique():\n",
    "        result_all=pd.DataFrame(columns=col)\n",
    "        df['temp_cluster']=df[dim_red_method]\n",
    "        df.loc[df[dim_red_method] != cluster, \"temp_cluster\"] = \"other_cluster\"\n",
    "        #print(df[[dim_red_method,'temp_cluster']])     \n",
    "        cat_columns=ctransformer.transformers[1][2]\n",
    "        #iterate over cat columns and calculate the p-Value: \n",
    "        for cat_col in cat_columns: \n",
    "            feature=cat_col\n",
    "            crosstab, test_results, expected = rp.crosstab(df[feature], df['temp_cluster'],\n",
    "                                                   test= \"chi-square\",\n",
    "                                                   expected_freqs= True,\n",
    "                                                   prop= \"cell\")\n",
    "            p=format(test_results[\"results\"][1], '.300000000g')\n",
    "            #print(p)\n",
    "           # if test_results[\"results\"][1]!=0:\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            compared=''\n",
    "            if feature !='gender':\n",
    "                feature_count_1=len(df.loc[df['temp_cluster']==cluster])\n",
    "                feature_cluster=df.loc[df['temp_cluster']==cluster]\n",
    "                feature_percentage_1=(len(feature_cluster.loc[feature_cluster[feature]=='Yes'])/feature_count_1)\n",
    "                #print(feature_percentage_1)\n",
    "    \n",
    "                feature_count_2=len(df.loc[df['temp_cluster']=='other_cluster'])\n",
    "                feature_cluster_2=df.loc[df['temp_cluster']=='other_cluster']\n",
    "                feature_percentage_2=(len(feature_cluster_2.loc[feature_cluster_2[feature]=='Yes'])/feature_count_2)\n",
    "                #print(feature_percentage_2)\n",
    "                if feature_percentage_1 > feature_percentage_2: \n",
    "                    compared='higher'\n",
    "                else:\n",
    "                    compared='lower'\n",
    "            row={'column name':(feature+'_'+cluster),'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            #row={'column name':feature,'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        for cat_col in sup_colums: \n",
    "            #print('Calculaint Supervised features')\n",
    "            feature=cat_col\n",
    "            crosstab, test_results, expected = rp.crosstab(df[feature], df['temp_cluster'],\n",
    "                                                   test= \"chi-square\",\n",
    "                                                   expected_freqs= True,\n",
    "                                                   prop= \"cell\")\n",
    "            #print(crosstab)\n",
    "            p=format(test_results[\"results\"][1], '.300000000g')\n",
    "            #print(p)\n",
    "           # if test_results[\"results\"][1]!=0:\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            compare=''\n",
    "            if feature !='gender':\n",
    "                feature_count_1=len(df.loc[df['temp_cluster']==cluster])\n",
    "                feature_cluster=df.loc[df['temp_cluster']==cluster]\n",
    "                feature_percentage_1=(len(feature_cluster.loc[feature_cluster[feature]=='Yes'])/feature_count_1)\n",
    "               # print(feature_percentage_1)\n",
    "    \n",
    "                feature_count_2=len(df.loc[df['temp_cluster']=='other_cluster'])\n",
    "                feature_cluster_2=df.loc[df['temp_cluster']=='other_cluster']\n",
    "                feature_percentage_2=(len(feature_cluster_2.loc[feature_cluster_2[feature]=='Yes'])/feature_count_2)\n",
    "               # print(feature_percentage_2)\n",
    "                if feature_percentage_1 > feature_percentage_2: \n",
    "                    compared='higher'\n",
    "                else:\n",
    "                    compared='lower'\n",
    "            row={'column name':(feature+'_'+cluster),'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            #row={'column name':feature,'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        result_all=result_all.sort_values(by=['absolute_p'],ascending=False)\n",
    "        result_chi.append(result_all)\n",
    "    #result_all=result_all.drop_duplicates(subset='column name',keep='first', inplace=False)\n",
    "    #return result_all.head(top_features)\n",
    "    return result_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_feature_importance_t_test(df,ctransformer,dim_red_method,n_cluster):\n",
    "    df_temp=df\n",
    "    #replace cluster names \n",
    "    for cluster in (range(n_cluster)):\n",
    "        cluster_name='cluster_'+str(cluster)\n",
    "        df[dim_red_method].replace({cluster: cluster_name},inplace=True)\n",
    "    #normalize num columns\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    num_columns=ctransformer.transformers[0][2]\n",
    "    #print(num_columns)\n",
    "    df_temp[list(num_columns)] = min_max_scaler.fit_transform(df_temp[list(num_columns)])\n",
    "    #iterate over num columns and calculate the p-Value: \n",
    "    col=['column name','T-Statistics','p-value','absolute_p','compared to other']\n",
    "    result_all=pd.DataFrame(columns=col) \n",
    "    result_t_test=[]\n",
    "    for cluster in df_temp[dim_red_method].unique():\n",
    "        result_all=pd.DataFrame(columns=col)\n",
    "        df_temp['temp_cluster']=df_temp[dim_red_method]\n",
    "        df_temp.loc[df[dim_red_method] != cluster, \"temp_cluster\"] = \"other_cluster\"\n",
    "        for num_col in num_columns: \n",
    "            feature=num_col\n",
    "            feature_value_1=df_temp.loc[df_temp['temp_cluster']==cluster][feature].values\n",
    "            feature_value_2=df_temp.loc[df_temp['temp_cluster']==\"other_cluster\"][feature].values\n",
    "            statistics,p=stats.ttest_ind(feature_value_1, feature_value_2, equal_var = False)\n",
    "            mean_1=feature_value_1.mean()\n",
    "            mean_2=feature_value_2.mean()\n",
    "            if mean_1 > mean_2: \n",
    "                compared='higher'\n",
    "            else:\n",
    "                compared='lower' \n",
    "           # print(feature_value_1)\n",
    "           # print(feature_value_2)\n",
    "           # print(p)\n",
    "            p=format(p, '.300000000g')\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            row={'column name':(feature+'_'+cluster),'T-Statistics':statistics,'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        result_all=result_all.sort_values(by=['absolute_p'],ascending=False)\n",
    "        result_t_test.append(result_all)        \n",
    "    return result_t_test\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subStringCluster(string):\n",
    "    a_string=string\n",
    "    split_string=a_string.split('_cluster_',1)\n",
    "    substring = split_string[0]\n",
    "    return substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopCluster(evaluation_pandas, n_topFeature, n_cluster ): \n",
    "    topFeatures=[]\n",
    "    for n in range(n_cluster):\n",
    "        #print(n)\n",
    "        features=[]\n",
    "         #categorical features\n",
    "        features=evaluation_pandas[1][n]['column name'].values\n",
    "        all_features = evaluation_pandas[1][n]\n",
    "        x=0\n",
    "        for i in range(n_topFeature):\n",
    "            feature=subStringCluster(features[x])\n",
    "            \n",
    "            if 'Procedure' in feature: \n",
    "               # print (feature)\n",
    "                #x=x+1\n",
    "                #print(subStringCluster(features[x]))\n",
    "                #topFeatures.append(subStringCluster(features[x]))\n",
    "                i=i-1\n",
    "            elif feature != 'gender' :\n",
    "                f=all_features.loc[all_features['column name']==features[x]]\n",
    "                p_value=f['p-value'].values\n",
    "                if p_value < 0.05 and p_value!=0.0 :\n",
    "                    topFeatures.append([subStringCluster(features[x]),'categorical'])\n",
    "                    #print(feature)\n",
    "                \n",
    "            else: \n",
    "                i=i-1\n",
    "            x=x+1\n",
    "        \n",
    "        #numeric\n",
    "        features=evaluation_pandas[2][n]['column name'].values\n",
    "        all_features = evaluation_pandas[2][n]\n",
    "        for i in range(n_topFeature):\n",
    "            f=all_features.loc[all_features['column name']==features[i]]\n",
    "            p_value=f['p-value'].values\n",
    "            if p_value < 0.05 and p_value!=0.0 :\n",
    "                topFeatures.append([subStringCluster(features[i]),'numeric'])\n",
    "    topFeatures_tuple=set(tuple(t)for t in topFeatures)\n",
    "    #print(topFeatures_tuple)\n",
    "    topFeatures=[t[0] for t in topFeatures_tuple]\n",
    "    #print(topFeatures)\n",
    "    #topFeatures=set(topFeatures)\n",
    "    #topFeatures=list(topFeatures)\n",
    "    #print(topFeatures)\n",
    "    return topFeatures_tuple, topFeatures\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/hpi-dhc/robotehr/blob/e3673aef701aa817c74d04170986f01fa191212a/robotehr/evaluation/risk_groups.py#L70-L100\n",
    "def plot_risk_groups(df, features,dim_red_method, friendly_names_converter=None, filename='', nrows=2, figsize=[12,3]):\n",
    "    #features=features[:2]\n",
    "    #ncols = int(len(features) / nrows)\n",
    "    ncols=1\n",
    "    #fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    fig, ax = plt.subplots(len(features),figsize=figsize)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    #print(len(features))\n",
    "    for i in range(len(features)):\n",
    "        #row_index = int(i / ncols)\n",
    "        row_index=i\n",
    "        #col_index = i % int(len(features) / nrows)\n",
    "        col_index=0\n",
    "\n",
    "        #current_axis = ax[row_index][col_index] if nrows > 1 else ax[col_index]\n",
    "        current_axis = ax[row_index]\n",
    "        if df[features[i]].min() == 0 and df[features[i]].max() == 1:\n",
    "            current_axis.set_ylim(bottom=-0.5, top=1.5)\n",
    "        sns.violinplot(\n",
    "            x=dim_red_method,\n",
    "            y=features[i],\n",
    "            data=df,\n",
    "            palette=\"muted\",\n",
    "            ax=current_axis,\n",
    "            hue='gender'\n",
    "        )\n",
    "        if friendly_names_converter:\n",
    "            title = friendly_names_converter.get(features[i])\n",
    "        else:\n",
    "            title = features[i]\n",
    "        if len(title) > 50:\n",
    "            title = f'{title[:50]} ...'\n",
    "        current_axis.set_title(f'{title}', fontsize=20)\n",
    "        current_axis.set_xlabel('')\n",
    "        current_axis.set_ylabel('')\n",
    "    if filename:\n",
    "        fig.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature_names(feature_evaluation):\n",
    "    feature_look_up=pq.read_table('Cohort/feature_look_up.parquet').to_pandas()\n",
    "    feature_evaluation['human_readable']=''\n",
    "    for index,r in feature_evaluation.iterrows():\n",
    "        lookuplist=feature_look_up['original_feature_name'].to_list()\n",
    "        if r['features'] in lookuplist:\n",
    "            human_readable_row=feature_look_up.loc[feature_look_up['original_feature_name']==r['features']]\n",
    "            human_readable=human_readable_row['human_readable'].values\n",
    "            #print(human_readable)\n",
    "            feature_evaluation.loc[feature_evaluation['features']==r['features'],'human_readable']=human_readable[0]\n",
    "        else :\n",
    "            feature_evaluation.loc[feature_evaluation['features']==r['features'],'human_readable']=r['features']\n",
    "    return feature_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overview_table(conv_df,features_tuples,features,dim_red_method): \n",
    "    feature_evaluation_df = pd.DataFrame()\n",
    "    feature_evaluation_df['features']=features\n",
    "    #print (feature_evaluation_df)\n",
    "    for cluster in conv_df[dim_red_method].unique(): \n",
    "        feature_evaluation_df[cluster]=0\n",
    "        cluster_df= conv_df.loc[conv_df[dim_red_method]==cluster]        \n",
    "        for features_tuple in features_tuples:\n",
    "            if features_tuple[1]=='categorical':\n",
    "                sum_feature=cluster_df[features_tuple[0]].sum()\n",
    "                percentage=sum_feature/len(cluster_df)\n",
    "                feature_evaluation_df.loc[feature_evaluation_df['features']==features_tuple[0],cluster]=percentage\n",
    "                \n",
    "                #print('categorical')\n",
    "            if features_tuple[1]=='numeric':\n",
    "                mean_feature=cluster_df[features_tuple[0]].mean()\n",
    "                median_feature=cluster_df[features_tuple[0]].median()\n",
    "                feature_evaluation_df.loc[feature_evaluation_df['features']==features_tuple[0],cluster]=(str(mean_feature)+'/'+str(median_feature))\n",
    "                #print('numeric') '''\n",
    "   # print(feature_evaluation_df)\n",
    "    return feature_evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTopFeatures(df_path,df,merge_w_supervised,dim_red_method, evaluation_results,  n_cluster, n_topFeatures): \n",
    "    '''#convert the dataframe\n",
    "    df_origin=pq.read_table(df_path).to_pandas()\n",
    "    #print(df_origin['gender'])\n",
    "    df_origin[dim_red_method]=df[dim_red_method]\n",
    "    conv_df=df_origin\n",
    "    if merge_w_supervised==True:\n",
    "        df_supervised_merge= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drugFORMerge.parquet').to_pandas()\n",
    "        conv_df.index = conv_df.index.map(str)\n",
    "        df_supervised_merge.index = df_supervised_merge.index.map(str)\n",
    "        conv_df=pd.merge(conv_df, df_supervised_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "     '''\n",
    "    conv_df=pq.read_table(df_path).to_pandas()\n",
    "    if merge_w_supervised==True:\n",
    "            df_supervised_merge= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drugFORMerge_wLab.parquet').to_pandas()\n",
    "            conv_df.index = conv_df.index.map(str)\n",
    "            df_supervised_merge.index = df_supervised_merge.index.map(str)\n",
    "            sup_colums=df_supervised_merge.columns\n",
    "            conv_df=pd.merge(conv_df, df_supervised_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "    \n",
    "    conv_df=conv_df.replace(True, 1)\n",
    "    conv_df=conv_df.replace(False,0)\n",
    "    conv_df=conv_df.replace('yes', 1)\n",
    "    conv_df=conv_df.replace('no',0)\n",
    "    conv_df=conv_df.fillna(0)\n",
    "    conv_df[dim_red_method]=df[dim_red_method]\n",
    "    conv_df=conv_df.sort_values(by=[dim_red_method],ascending=True)\n",
    "    #get top featrues: \n",
    "    evaluation_pandas=evaluation_results\n",
    "    features_tuples,features=getTopCluster(evaluation_pandas, n_topFeatures, n_cluster)\n",
    "    #plot features \n",
    "    #print (cluster_name)\n",
    "   # fig_x=12*len(features)\n",
    "    fig_y=8*len(features)\n",
    "    plot_risk_groups(conv_df, features, dim_red_method,friendly_names_converter=None, filename='', nrows=10, figsize=[12,fig_y])\n",
    "    feature_evaluation_df=create_overview_table(conv_df,features_tuples,features,dim_red_method)\n",
    "    feature_evaluation_df=map_feature_names(feature_evaluation_df)\n",
    "    return feature_evaluation_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_LSTM_Pipeline(per_day,embedding_method,embedding_size,cluster_methods,n_cluster,ellbow_method,plot,boxplot,feature_evaluation,merge_w_supervised): \n",
    "    #load Data\n",
    "    data,data_sample, df_name,data_structure=load_dataset(per_day,embedding_method,embedding_size)\n",
    "    timesteps=len(data[0])\n",
    "    n_features=len(data[0][0]) \n",
    "    #load model and compromise data dimension\n",
    "    dim_red_data=load_lstm_representation(data_structure,timesteps,n_features,layer_size_1,layer_size_2,activation_func,optimizer_func,loss_func,n_epochs,n_batch_size,df_name,data)\n",
    "      \n",
    "\n",
    "    #apply cluster method: \n",
    "    if cluster_methods=='kmeans':\n",
    "        labels=apply_kmeans(dim_red_data,n_cluster,ellbow_method)   \n",
    "    if cluster_methods=='hierachical':\n",
    "        labels=apply_hierachical(dim_red_data,n_cluster)\n",
    "    #plott cluster results\n",
    "    if plot==True: \n",
    "        plot_data=apply_umap(dim_red_data,2)\n",
    "        scatter_plot(plot_data,None)\n",
    "        scatter_plot(plot_data,labels)\n",
    "    #apply metrics \n",
    "    silhouette_Coefficient= get_silhouette_Coefficient(labels,dim_red_data)\n",
    "    calinski_harabasz=get_calinski_harabasz(labels,dim_red_data)\n",
    "    davies_bouldin=get_davies_bouldin(labels,dim_red_data)\n",
    "    if per_day== True: \n",
    "        data_structure='per_day'\n",
    "    result=pq.read_table('Cohort/LSTM_Models/Evaluation_LSTM.parquet').to_pandas()\n",
    "    result=result.append({'df_name':df_name,'data_structure':data_structure,'timesteps':timesteps,'n_features':n_features,'layer_size_1':layer_size_1,'layer_size_2':layer_size_2,'activation_func':activation_func,'optimizer_func':optimizer_func,'loss_func':loss_func,'n_epochs':n_epochs,'n_batch_size':n_batch_size,'cluster_method':cluster_methods,'n_cluster':n_cluster,'silhouette_Coefficient':silhouette_Coefficient,'calinski_harabasz':calinski_harabasz,'davies_bouldin':davies_bouldin}, ignore_index=True)\n",
    "    result.to_parquet('Cohort/LSTM_Models/Evaluation_LSTM.parquet')\n",
    "    print(result)\n",
    "    if per_day== True: \n",
    "        data_structure='per_patient_per_day'\n",
    "    #load static dataset for calculating features importaces\n",
    "    if data_structure=='per_patient':\n",
    "        time_series_static=pq.read_table('Cohort/Time_Series/time_series_per_patient_static_features.parquet').to_pandas()\n",
    "    if data_structure=='per_patient_per_day':\n",
    "        time_series_static=pq.read_table('Cohort/Time_Series/time_series_per_patient_per_day_static_features.parquet').to_pandas()\n",
    "        print(len(time_series_static))\n",
    "        print(len(labels))\n",
    "    #sample\n",
    "    #time_series_static=time_series_static.head(300)\n",
    "    if merge_w_supervised==True:\n",
    "            df_supervised_merge= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drugFORMerge_wLab.parquet').to_pandas()\n",
    "            time_series_static.index = time_series_static.index.map(str)\n",
    "            df_supervised_merge.index = df_supervised_merge.index.map(str)\n",
    "            sup_colums=df_supervised_merge.columns\n",
    "            time_series_static=pd.merge(time_series_static, df_supervised_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "    else:\n",
    "        sup_colums=[]  \n",
    "    #attach labels to static feature set:\n",
    "    time_series_static['label']=labels\n",
    "    #apply age filter: \n",
    "    time_series_static.loc[(time_series_static['HF_Onset_age_in_days'] > 32850),'HF_Onset_age_in_days']=32850\n",
    "    evaluation_results=[]\n",
    "    cluster_information=get_cluster_statistics(time_series_static,'label',boxplot)\n",
    "    evaluation_results.append(cluster_information) \n",
    "    if feature_evaluation == True:\n",
    "         #load ctansformer: \n",
    "        with open('Cohort/Models/ColumnTransformer/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab_MinMaxScaler_BinaryEncoder_woGender_woAge.pkl', 'rb') as f:\n",
    "            ctransformer = pickle.load(f)\n",
    "        print('ctransformer loaded')\n",
    "        top_catigorical_features=cat_feature_importance(time_series_static,ctransformer,sup_colums,'label',n_cluster)\n",
    "        print('Top Categorical features: \\n',top_catigorical_features)\n",
    "        evaluation_results.append(top_catigorical_features)\n",
    "        \n",
    "        \n",
    "        top_numerical_features_t_test=num_feature_importance_t_test(time_series_static,ctransformer,'label',n_cluster)\n",
    "        print('Top Numerical features: \\n',top_numerical_features_t_test)\n",
    "        evaluation_results.append(top_numerical_features_t_test) \n",
    "    return evaluation_results,time_series_static\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_day=False\n",
    "embedding_method='cbow'\n",
    "embedding_size='20'\n",
    "boxplot=True\n",
    "#Model\n",
    "#data_structure='per_patient'#'per_patient_per_day'\n",
    "\n",
    "layer_size_1=32\n",
    "layer_size_2=16\n",
    "activation_func='sigmoid'\n",
    "optimizer_func='adam'\n",
    "loss_func='mse'\n",
    "n_epochs=300\n",
    "n_batch_size=1024\n",
    "#CLuster\n",
    "n_cluster=3\n",
    "ellbow_method=False\n",
    "cluster_methods='kmeans'\n",
    "plot=True\n",
    "merge_w_supervised=True\n",
    "feature_evaluation=True\n",
    "evaluation_results,time_series_static=cluster_LSTM_Pipeline(per_day,embedding_method,embedding_size,cluster_methods,n_cluster,ellbow_method,plot,boxplot,feature_evaluation,merge_w_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if per_day==False:\n",
    "        df_path='Cohort/Time_Series/time_series_per_patient_static_features.parquet'\n",
    "if per_day==True:\n",
    "        df_path='Cohort/Time_Series/time_series_per_patient_per_day_static_features.parquet'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_evaluation_df=plotTopFeatures(df_path,time_series_static,merge_w_supervised,'label', evaluation_results,n_cluster ,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_evaluation_df.to_excel(\"Cohort/LSTM_Models/Feature_evaluation_\"+activation_func+optimizer_func+loss_func+str(layer_size_1)+str(layer_size_2)+cluster_methods+\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Inpatient p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load inpatient and EF dataframe \n",
    "hospitalization = pq.read_table('Cohort/Feature_Extraction/days_in_hospital.parquet').to_pandas()\n",
    "ef=pq.read_table('Cohort/Feature_Extraction/avg_EF.parquet').to_pandas()\n",
    "#merge both to the df: \n",
    "df_cohort=pd.merge(time_series_static, hospitalization, how='left', left_index=True, right_on='medical_record_number')\n",
    "df_cohort=pd.merge(df_cohort, ef, how='left',left_index=True, right_on='medical_record_number')\n",
    "df_cohort['days_in_hospital']=df_cohort['days_in_hospital'].fillna(0)\n",
    "#get average days in hospital per patient per cluster\n",
    "cl_0=df_cohort.loc[df_cohort['temp_cluster']=='cluster_1']\n",
    "cl_1=df_cohort.loc[df_cohort['temp_cluster']=='other_cluster']\n",
    "#cl_2=df_cohort.loc[df_cohort['label']=='cluster_2']\n",
    "t=t_test_avg_ef=stats.ttest_ind(cl_0['days_in_hospital'], cl_1['days_in_hospital'])\n",
    "t\n",
    "#cl_0['days_in_hospital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_static['min__LabValue__QT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTopFeatures(time_series_static,'label', evaluation_results,n_cluster ,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop \n",
    "embedding_method='cbow'\n",
    "embedding_size='20'\n",
    "boxplot=False\n",
    "#size= [(32,16),(128,64)]\n",
    "size= [(100,50)]\n",
    "arr_per_day=[ False,True]#[False,True]\n",
    "#arr_activation_function=['tanh','sigmoid','relu']\n",
    "arr_activation_function=['tanh','sigmoid','relu']\n",
    "optimizer_func='adam'\n",
    "loss_func='mse'\n",
    "n_epochs=300\n",
    "n_batch_size=1024\n",
    "arr_n_cluster=[3,4,5]\n",
    "ellbow_method=False\n",
    "plot=False\n",
    "merge_w_supervised=True\n",
    "t=0\n",
    "arr_cluster_methods=['kmeans','hierachical']\n",
    "for q in arr_per_day: \n",
    "    per_day=q\n",
    "    for s in size: \n",
    "        layer_size_1=s[0]\n",
    "        layer_size_2=s[1]\n",
    "        for activation in arr_activation_function: \n",
    "            activation_func=activation\n",
    "            for c in arr_cluster_methods: \n",
    "                    cluster_methods=c\n",
    "                    t=t+1\n",
    "                    print(t)\n",
    "                    for n in arr_n_cluster: \n",
    "                        n_cluster=n \n",
    "                        evaluation_results=cluster_LSTM_Pipeline(per_day,embedding_method,embedding_size,cluster_methods,n_cluster,ellbow_method,plot,boxplot,merge_w_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_day=True\n",
    "embedding_method='cbow'\n",
    "embedding_size='20'\n",
    "boxplot=True\n",
    "#Model\n",
    "#data_structure='per_patient'#'per_patient_per_day'\n",
    "\n",
    "layer_size_1=32\n",
    "layer_size_2=16\n",
    "activation_func='relu'\n",
    "optimizer_func='adam'\n",
    "loss_func='mse'\n",
    "n_epochs=300\n",
    "n_batch_size=1024\n",
    "#CLuster\n",
    "n_cluster=3\n",
    "ellbow_method=True\n",
    "cluster_methods='kmeans'\n",
    "plot=True\n",
    "merge_w_supervised=True\n",
    "evaluation_results=cluster_LSTM_Pipeline(per_day,embedding_method,embedding_size,cluster_methods,n_cluster,ellbow_method,plot,boxplot,feature_evaluation,merge_w_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create blank evaluation excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['df_name','data_structure','timesteps','n_features','layer_size_1','layer_size_2','activation_func','optimizer_func','loss_func','n_epochs','n_batch_size','cluster_method','n_cluster','silhouette_Coefficient','calinski_harabasz','davies_bouldin','silhouette_Coefficient_static','calinski_harabasz_static','davies_bouldin_static']\n",
    "result=pd.DataFrame(columns=col)\n",
    "#result.to_parquet('Cohort/LSTM_Models/Evaluation_LSTM.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=pq.read_table('Cohort/LSTM_Models/Evaluation_LSTM.parquet').to_pandas()\n",
    "result.sort_values(by=['silhouette_Coefficient','data_structure','activation_func','layer_size_2'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "result=result.sort_values(['silhouette_Coefficient','data_structure','activation_func','layer_size_2'], ascending = False )\n",
    "s = result.style.background_gradient(cmap=cm)\n",
    "s\n",
    "#s.to_excel(\"Cohort/LSTM_Models/11_04_Evaluation_cluster.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table with LSTM Model Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=pq.read_table('Cohort/Metrics_LSTM.parquet').to_pandas()\n",
    "a=a.sort_values(by=['data_structure','loss'])\n",
    "#a=\n",
    "a.loc[a['timesteps']==7,'data_structure']='per_day'\n",
    "a=a.sort_values(by=['data_structure','loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.style.background_gradient(cmap=cm)\n",
    "a\n",
    "a.to_excel(\"Cohort/LSTM_Models/11_04_Evaluation_loss.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s= [(32,16),(128,64)]\n",
    "for e in s: \n",
    "    print(e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  time_series_static=pq.read_table('Cohort/Time_Series/time_series_per_patient_static_features.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
