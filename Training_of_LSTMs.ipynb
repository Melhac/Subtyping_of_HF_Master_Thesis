{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of LSTM \n",
    "\n",
    "This Notebook is embedding the sequences of the Patients. These embedded sequences are then used for the Training of various LSTM models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm autoencoder to recreate a timeseries\n",
    "#gridsearch: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "#https://machinelearningmastery.com/how-to-grid-search-deep-learning-models-for-time-series-forecasting/\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "import pyarrow.parquet as pq\n",
    "from gensim.models import Word2Vec\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn import preprocessing\n",
    "##### REQUIRES THE DATAFRAME FOLDER TO BE NAMED 'Cohorts', WHICH INCLUDES ALL PRECOMPUTED DATAFRAMES #####\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from functools import reduce\n",
    "from ppca import PPCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "import pickle\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import scipy.stats as stats\n",
    "import researchpy as rp\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from keras.layers import Input, Dense \n",
    "from keras.models import Model, Sequential \n",
    "from keras import regularizers \n",
    "import umap\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embedded sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timeseries_per_patient_LSTM_Data_embedded_cbow_dim20_win5_mc0.txt\n",
    "def load_dataset(per_day,embedding_method,embedding_size):\n",
    "    if per_day: \n",
    "        df_name='Timeseries_per_patient_per_day_LSTM_Data_embedded_'\n",
    "    else: \n",
    "        df_name='Timeseries_per_patient_LSTM_Data_embedded_'\n",
    "    if embedding_method=='cbow':\n",
    "        df_name=df_name+'cbow_dim{}_win5_mc0'.format(embedding_size)\n",
    "    if embedding_method=='skipgram':\n",
    "        df_name=df_name+'skipgram_dim{}_win5_mc0'.format(embedding_size)\n",
    "    print(df_name)\n",
    "    with open(\"Cohort/Time_Series/\"+df_name+'.txt', \"rb\") as fp:   # Unpickling\n",
    "        data = pickle.load(fp)\n",
    "    data_sample= data[:300]\n",
    "    return data,data_sample, df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure dataset that should be used \n",
    "per_day=False\n",
    "embedding_method='cbow'\n",
    "embedding_size='20'\n",
    "\n",
    "data,sample,df_name=load_dataset(per_day,embedding_method,embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure='per_patient'#'per_patient_per_day'\n",
    "timesteps=len(data[0])\n",
    "n_features=len(data[0][0])\n",
    "layer_size_1=32\n",
    "layer_size_2=16\n",
    "activation_func='tanh'\n",
    "optimizer_func='adam'\n",
    "loss_func='mse'\n",
    "n_epochs=3\n",
    "n_batch_size=100\n",
    "X=sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(layer_size_1, activation=activation_func, input_shape=(timesteps,n_features), return_sequences=True))\n",
    "model.add(LSTM(layer_size_2, activation=activation_func, return_sequences=False))\n",
    "model.add(RepeatVector(timesteps))\n",
    "model.add(LSTM(layer_size_2, activation=activation_func, return_sequences=True))\n",
    "model.add(LSTM(layer_size_1, activation=activation_func, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer=optimizer_func, loss=loss_func,metrics=[loss_func])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "\n",
    "\n",
    "hist=model.fit(X, X, epochs=n_epochs, batch_size=n_batch_size, verbose=1)\n",
    "# demonstrate reconstruction\n",
    "#print(history.history['val_loss'][(epochs-1)])\n",
    "yhat = model.predict(X, verbose=0)\n",
    "print('---Predicted---')\n",
    "print(np.round(yhat,3))\n",
    "print('---Actual---')\n",
    "print(np.round(X, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value=hist.history[loss_func][n_epochs-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pq.read_table('Cohort/Metrics_LSTM.parquet').to_pandas()\n",
    "result=result.append({'data_structure':data_structure,'timesteps':timesteps,'n_features':n_features,'layer_size_1':layer_size_1,'layer_size_2':layer_size_2,'activation_func':activation_func,'optimizer_func':optimizer_func,'loss_func':loss_func,'n_epochs':n_epochs,'n_batch_size':n_batch_size,'loss':loss_value}, ignore_index=True)\n",
    "result.to_parquet('Cohort/Metrics_LSTM.parquet')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pq.read_table('Cohort/Metrics_LSTM.parquet').to_pandas()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=df_name+'_'+data_structure+str(layer_size_1)+'_'+str(layer_size_2)+'_'+activation_func+'_'+optimizer_func+'_'+loss_func+'_'+str(n_epochs)+'_'+str(n_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Cohort/LSTM_Models/'+experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Test for  prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('Cohort/LSTM_Models/'+experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_representation = Sequential() \n",
    "hidden_representation.add(model.layers[0]) \n",
    "hidden_representation.add(model.layers[1])  \n",
    "#hidden_representation.add(model.layers[2])\n",
    "normal_hidden_rep = hidden_representation.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_hidden_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normal_hidden_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normal_hidden_rep[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mrns  can\n",
    "timeseries_per_patient = pq.read_table('Cohort/Time_Series/time_series_per_patient_mrns.parquet').to_pandas()\n",
    "#timeseries_per_patient=timeseries_per_patient.drop('unique_concept', axis=1)\n",
    "#timeseries_per_patient\n",
    "timeseries_per_patient_sample=timeseries_per_patient.head(300)\n",
    "#timeseries_per_patient_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrn=timeseries_per_patient['medical_record_number'].to_list()\n",
    "mrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_umap(transformed_df,dimension):\n",
    "    clusterable_embedding = umap.UMAP(\n",
    "        n_neighbors=50,\n",
    "        min_dist=0.1,\n",
    "        n_components=dimension,\n",
    "        random_state=42,\n",
    "    )\n",
    "    X_transformed=clusterable_embedding.fit_transform(transformed_df)\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(df,labels):\n",
    "    sns.set(style='white', rc={'figure.figsize':(10,8)})\n",
    "    plt.scatter(df[:, 0], df[:, 1], c=labels, s=0.1, cmap='Spectral');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kmeans(transformed_sample,ellbow_method,cluster):\n",
    "    if ellbow_method==True:\n",
    "        elbow_method(transformed_sample)\n",
    "    #scatter_plot(transformed_sample,None) \n",
    "    #plt.scatter(transformed_sample[:,0],transformed_sample[:,1])\n",
    "    kmeans = KMeans(n_clusters=cluster, init='k-means++', max_iter=5000, n_init=10, random_state=0)\n",
    "    pred_y = kmeans.fit_predict(transformed_sample)\n",
    "    #plt.scatter(transformed_sample[:,0], transformed_sample[:,1])\n",
    "    #plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
    "    #plt.show()\n",
    "    #scatter_plot(transformed_sample,kmeans.labels_)\n",
    "    '''\n",
    "    from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\n",
    "    colors = ['#4EACC5', '#FF9C34', '#4E9A06','#FF0000','#8800FF']\n",
    "    k_means_labels = pairwise_distances_argmin(transformed_sample, kmeans.cluster_centers_)\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    for k, col in zip(range(cluster), colors):\n",
    "        my_members = k_means_labels == k\n",
    "        cluster_center = kmeans.cluster_centers_[k]\n",
    "        ax.plot(transformed_sample[my_members, 0], transformed_sample[my_members, 1], 'w',\n",
    "                markerfacecolor=col, marker='.')\n",
    "        ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "                markeredgecolor='k', markersize=6)\n",
    "    experiment_name=experiment_name\n",
    "    ax.set_title(experiment_name)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    fig.savefig('Cohort/Models/Plots/'+experiment_name+'.png')'''\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(transformed_sample): \n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(transformed_sample)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(1, 11), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_silhouette_Coefficient(labels,df):\n",
    "    m=metrics.silhouette_score(df, labels, metric='euclidean')\n",
    "    print('silhouette_score:',m)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster= 4 \n",
    "#prepare data for plotting \n",
    "df_dim_red_plot=apply_umap(normal_hidden_rep,2)        \n",
    "#print first 2 dim of dimensionality reduced data:\n",
    "scatter_plot(df_dim_red_plot,None)\n",
    "labels=apply_kmeans(normal_hidden_rep,True,n_cluster)\n",
    "scatter_plot(df_dim_red_plot,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_silhouette_Coefficient(labels,normal_hidden_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for statistics: \n",
    "def get_base_characteristic_value(df , characteristic , kind):    \n",
    "    if kind==\"mean\": \n",
    "        df_mean=df[characteristic].mean()\n",
    "        df_std= df[characteristic].std()\n",
    "        df_max= df[characteristic].max()\n",
    "        df_min= df[characteristic].min()\n",
    "        base_characteristics_cohort=pd.DataFrame({'Variable': [characteristic+\"_mean\", characteristic+\"_std\", characteristic+\"_max\", characteristic+\"_min\"],\n",
    "                                                  'Value': [(df_mean/365), (df_std/365), (df_max/365), (df_min/365)],})\n",
    "       \n",
    "    if kind==\"count\":\n",
    "        base_characteristics_cohort=pd.DataFrame(columns=[\"Variable\",\"Value\"])\n",
    "        feature_value=df[characteristic].unique()\n",
    "        #print(feature_value)\n",
    "        for value in feature_value: \n",
    "            df_condition=df.loc[df[characteristic]==value]\n",
    "            df_percent= df_condition.shape[0]/df.shape[0]\n",
    "            #print(df_percent)\n",
    "            new_row1 = {'Variable': value+\"_total\",'Value': df_condition.shape[0]}\n",
    "            new_row2 = {'Variable': value+\"_relation\",'Value': df_percent}\n",
    "            base_characteristics_cohort=base_characteristics_cohort.append(new_row1, ignore_index=True)\n",
    "            base_characteristics_cohort=base_characteristics_cohort.append(new_row2, ignore_index=True)\n",
    "       # print(df_condition.shape[0], df_percent)\n",
    "    #print (base_characteristics_cohort)\n",
    "    return base_characteristics_cohort\n",
    "\n",
    "def get_base_characteristics(df, characteristics): \n",
    "    base_characteristics_cohort=pd.DataFrame(columns=[\"Variable\",\"Value\"])\n",
    "    for characteristic in characteristics:\n",
    "        intermediate_base_characteristics_cohort=get_base_characteristic_value(df,characteristic[0],characteristic[1])\n",
    "        base_characteristics_cohort=pd.concat([base_characteristics_cohort,intermediate_base_characteristics_cohort])\n",
    "    print(base_characteristics_cohort)\n",
    "    return base_characteristics_cohort\n",
    "\n",
    "def get_cluster_information(df):\n",
    "    dim_red_method=df\n",
    "    dim_red_method='LSTM_labels'\n",
    "    base_characteristics=[\n",
    "        [ \"HF_Onset_age_in_days\",\"mean\"],\n",
    "        [\"gender\",\"count\"]\n",
    "        ]\n",
    "    baseline_characteristics=[]\n",
    "    for cluster in df[dim_red_method].unique(): \n",
    "        cluster_characteristics=[]\n",
    "        df_temp=df.loc[df[dim_red_method] == cluster]\n",
    "        df_base_characteristics=get_base_characteristics(df_temp, base_characteristics)\n",
    "        \n",
    "        cluster_characteristics.append(cluster)\n",
    "        cluster_characteristics.append(len(df_temp))\n",
    "        cluster_characteristics.append(df_base_characteristics)\n",
    "        baseline_characteristics.append(cluster_characteristics)\n",
    "    return baseline_characteristics\n",
    "        #print(str(cluster))\n",
    "        #print(len(df_temp))\n",
    "        \n",
    "        #print(df_temp_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_feature_importance_anova(df,ctransformer,dim_red_method,n_cluster,top_features):\n",
    "    df_temp=df\n",
    "    #replace cluster names \n",
    "    for cluster in (range(n_cluster)):\n",
    "        cluster_name='cluster_'+str(cluster)\n",
    "        df[dim_red_method].replace({cluster: cluster_name},inplace=True)\n",
    "    #normalize num columns\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    num_columns=ctransformer.transformers[0][2]\n",
    "    df_temp[list(num_columns)] = min_max_scaler.fit_transform(df_temp[list(num_columns)])\n",
    "    #iterate over num columns and calculate the p-Value: \n",
    "    col=['column name','F-Value','p-value','absolute_p','compared to other']\n",
    "    result_all=pd.DataFrame(columns=col) \n",
    "    result_anova=[]\n",
    "    for cluster in df_temp[dim_red_method].unique():\n",
    "        result_all=pd.DataFrame(columns=col)\n",
    "        df_temp['temp_cluster']=df_temp[dim_red_method]\n",
    "        df_temp.loc[df[dim_red_method] != cluster, \"temp_cluster\"] = \"other_cluster\"\n",
    "        for num_col in num_columns: \n",
    "            feature=num_col\n",
    "            result = df_temp.groupby('temp_cluster')[feature].apply(list)\n",
    "            #print(result)\n",
    "            feature_value_1=result[cluster]\n",
    "            #print(feature_value_1)\n",
    "            feature_value_2=result['other_cluster']\n",
    "            mean_1=mean(feature_value_1)\n",
    "            mean_2=mean(feature_value_2)\n",
    "            if mean_1 > mean_2: \n",
    "                compared='higher'\n",
    "            else:\n",
    "                compared='lower'\n",
    "            #print(len(result['cluster_3']))\n",
    "            #print(len(result['cluster_0']))\n",
    "            F, p = stats.f_oneway(*result)\n",
    "            p=format(p, '.300000000g')\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            row={'column name':(feature+'_'+cluster),'F-Value':F,'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        result_all=result_all.sort_values(by=['absolute_p'],ascending=False)\n",
    "        result_anova.append(result_all)\n",
    "    #result_all=result_all.drop_duplicates(subset='column name',keep='first', inplace=False)\n",
    "    #return result_all.head(top_features)\n",
    "    return result_anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_feature_importance(df,ctransformer,sup_colums,dim_red_method,n_cluster,top_features):\n",
    "    #replace cluster names \n",
    "    #establish two categories in all Categories \n",
    "    \n",
    "   \n",
    "    for cluster in (range(n_cluster)):\n",
    "        cluster_name='cluster_'+str(cluster)\n",
    "        df[dim_red_method].replace({cluster: cluster_name},inplace=True)\n",
    "    df=df.replace(True, 'Yes')\n",
    "    df=df.replace(False,'No')\n",
    "    df=df.fillna('No')\n",
    "    df=df.replace(1, 'Yes')\n",
    "    df=df.replace(0,'No')\n",
    "    df=df.fillna('No')\n",
    "    col=['column name','Pearson Chi-square','Cramers V','p-value','absolute_p','compared to other']\n",
    "    result_all=pd.DataFrame(columns=col)\n",
    "    result_chi=[]\n",
    "    for cluster in df[dim_red_method].unique():\n",
    "        result_all=pd.DataFrame(columns=col)\n",
    "        df['temp_cluster']=df[dim_red_method]\n",
    "        df.loc[df[dim_red_method] != cluster, \"temp_cluster\"] = \"other_cluster\"\n",
    "        #print(df[[dim_red_method,'temp_cluster']])     \n",
    "        cat_columns=ctransformer.transformers[1][2]\n",
    "        #iterate over cat columns and calculate the p-Value: \n",
    "        for cat_col in cat_columns: \n",
    "            feature=cat_col\n",
    "            crosstab, test_results, expected = rp.crosstab(df[feature], df['temp_cluster'],\n",
    "                                                   test= \"chi-square\",\n",
    "                                                   expected_freqs= True,\n",
    "                                                   prop= \"cell\")\n",
    "            p=format(test_results[\"results\"][1], '.300000000g')\n",
    "            #print(p)\n",
    "           # if test_results[\"results\"][1]!=0:\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            compared=''\n",
    "            if feature !='gender':\n",
    "                feature_count_1=len(df.loc[df['temp_cluster']==cluster])\n",
    "                feature_cluster=df.loc[df['temp_cluster']==cluster]\n",
    "                feature_percentage_1=(len(feature_cluster.loc[feature_cluster[feature]=='Yes'])/feature_count_1)\n",
    "                #print(feature_percentage_1)\n",
    "    \n",
    "                feature_count_2=len(df.loc[df['temp_cluster']=='other_cluster'])\n",
    "                feature_cluster_2=df.loc[df['temp_cluster']=='other_cluster']\n",
    "                feature_percentage_2=(len(feature_cluster_2.loc[feature_cluster_2[feature]=='Yes'])/feature_count_2)\n",
    "                #print(feature_percentage_2)\n",
    "                if feature_percentage_1 > feature_percentage_2: \n",
    "                    compared='higher'\n",
    "                else:\n",
    "                    compared='lower'\n",
    "            row={'column name':(feature+'_'+cluster),'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            #row={'column name':feature,'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        for cat_col in sup_colums: \n",
    "            feature=cat_col\n",
    "            crosstab, test_results, expected = rp.crosstab(df[feature], df['temp_cluster'],\n",
    "                                                   test= \"chi-square\",\n",
    "                                                   expected_freqs= True,\n",
    "                                                   prop= \"cell\")\n",
    "            #print(crosstab)\n",
    "            p=format(test_results[\"results\"][1], '.300000000g')\n",
    "            #print(p)\n",
    "           # if test_results[\"results\"][1]!=0:\n",
    "            p=float(p)\n",
    "            if p!=0:\n",
    "\n",
    "                importance=abs(np.log(p))\n",
    "            else: \n",
    "                importance=0\n",
    "            compare=''\n",
    "            if feature !='gender':\n",
    "                feature_count_1=len(df.loc[df['temp_cluster']==cluster])\n",
    "                feature_cluster=df.loc[df['temp_cluster']==cluster]\n",
    "                feature_percentage_1=(len(feature_cluster.loc[feature_cluster[feature]=='Yes'])/feature_count_1)\n",
    "               # print(feature_percentage_1)\n",
    "    \n",
    "                feature_count_2=len(df.loc[df['temp_cluster']=='other_cluster'])\n",
    "                feature_cluster_2=df.loc[df['temp_cluster']=='other_cluster']\n",
    "                feature_percentage_2=(len(feature_cluster_2.loc[feature_cluster_2[feature]=='Yes'])/feature_count_2)\n",
    "               # print(feature_percentage_2)\n",
    "                if feature_percentage_1 > feature_percentage_2: \n",
    "                    compared='higher'\n",
    "                else:\n",
    "                    compared='lower'\n",
    "            row={'column name':(feature+'_'+cluster),'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance,'compared to other':compared}\n",
    "            #row={'column name':feature,'Pearson Chi-square':test_results[\"results\"][0],'Cramers V':test_results[\"results\"][2],'p-value':p,'absolute_p':importance}\n",
    "            result_all=result_all.append(row, ignore_index=True)\n",
    "        result_all=result_all.sort_values(by=['absolute_p'],ascending=False)\n",
    "        result_chi.append(result_all)\n",
    "    #result_all=result_all.drop_duplicates(subset='column name',keep='first', inplace=False)\n",
    "    #return result_all.head(top_features)\n",
    "    return result_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTopFeatures(df,merge_w_supervised,dim_red_method, evaluation_results,  n_cluster, n_topFeatures): \n",
    "    #convert the dataframe\n",
    "    df_origin=df\n",
    "   # df_origin=pq.read_table(df_path).to_pandas()\n",
    "    #print(df_origin['gender'])\n",
    "    df_origin[dim_red_method]=df[dim_red_method]\n",
    "    conv_df=df_origin\n",
    "    if merge_w_supervised==True:\n",
    "        df_supervised_merge= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drugFORMerge.parquet').to_pandas()\n",
    "        conv_df.index = conv_df.index.map(str)\n",
    "        df_supervised_merge.index = df_supervised_merge.index.map(str)\n",
    "        conv_df=pd.merge(conv_df, df_supervised_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "        \n",
    "    conv_df=conv_df.replace(True, 1)\n",
    "    conv_df=conv_df.replace(False,0)\n",
    "    conv_df=conv_df.replace('yes', 1)\n",
    "    conv_df=conv_df.replace('no',0)\n",
    "    conv_df=conv_df.fillna(0)\n",
    "    conv_df=conv_df.sort_values(by=[dim_red_method],ascending=True)\n",
    "    #get top featrues: \n",
    "    evaluation_pandas=evaluation_results\n",
    "    features=getTopCluster(evaluation_pandas, n_topFeatures, n_cluster)\n",
    "    #plot features \n",
    "    #print (cluster_name)\n",
    "    plot_risk_groups(conv_df, features, dim_red_method,friendly_names_converter=None, filename='', nrows=1, figsize=[24,10])\n",
    "    return features\n",
    "#https://github.com/hpi-dhc/robotehr/blob/e3673aef701aa817c74d04170986f01fa191212a/robotehr/evaluation/risk_groups.py#L70-L100\n",
    "def plot_risk_groups(df, features,dim_red_method, friendly_names_converter=None, filename='', nrows=1, figsize=[12,3]):\n",
    "    ncols = int(len(features) / nrows)\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        row_index = int(i / ncols)\n",
    "        col_index = i % int(len(features) / nrows)\n",
    "\n",
    "        current_axis = ax[row_index][col_index] if nrows > 1 else ax[col_index]\n",
    "        if df[features[i]].min() == 0 and df[features[i]].max() == 1:\n",
    "            current_axis.set_ylim(bottom=-0.5, top=1.5)\n",
    "        sns.violinplot(\n",
    "            x=dim_red_method,\n",
    "            y=features[i],\n",
    "            data=df,\n",
    "            palette=\"muted\",\n",
    "            ax=current_axis,\n",
    "            #hue='gender'\n",
    "        )\n",
    "        if friendly_names_converter:\n",
    "            title = friendly_names_converter.get(features[i])\n",
    "        else:\n",
    "            title = features[i]\n",
    "        if len(title) > 50:\n",
    "            title = f'{title[:50]} ...'\n",
    "        current_axis.set_title(f'{title}', fontsize=11)\n",
    "        current_axis.set_xlabel('')\n",
    "        current_axis.set_ylabel('')\n",
    "    if filename:\n",
    "        fig.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    return fig\n",
    "def getTopCluster(evaluation_pandas, n_topFeature, n_cluster ): \n",
    "    topFeatures=[]\n",
    "    for n in range(n_cluster):\n",
    "        print(n)\n",
    "        features=[]\n",
    "         #categorical features\n",
    "        features=evaluation_pandas[2][n]['column name'].values\n",
    "        x=0\n",
    "        for i in range(n_topFeature):\n",
    "            feature=subStringCluster(features[x])\n",
    "            if feature != 'gender' :\n",
    "                topFeatures.append(subStringCluster(features[x]))\n",
    "            if 'Procedure' in feature: \n",
    "                print (feature)\n",
    "                x=x+1\n",
    "                print(subStringCluster(features[x]))\n",
    "                topFeatures.append(subStringCluster(features[x]))\n",
    "                \n",
    "            else: \n",
    "                x=x+1\n",
    "                #print(features)\n",
    "                topFeatures.append(subStringCluster(features[x]))\n",
    "            x=x+1\n",
    "        \n",
    "        #numeric\n",
    "        features=evaluation_pandas[1][n]['column name'].values \n",
    "        for i in range(n_topFeature):\n",
    "            topFeatures.append(subStringCluster(features[n]))\n",
    "    topFeatures=set(topFeatures)\n",
    "    topFeatures=list(topFeatures)\n",
    "    print(topFeatures)\n",
    "    return topFeatures\n",
    "def subStringCluster(string):\n",
    "    a_string=string\n",
    "    split_string=a_string.split('_cluster_',1)\n",
    "    substring = split_string[0]\n",
    "    return substring\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Cluster insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# load static feature Cohort: \n",
    "df_origin= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned_wLab.parquet').to_pandas()\n",
    "df_origin.loc[(df_origin['HF_Onset_age_in_days'] > 32850),'HF_Onset_age_in_days']=32850\n",
    "#merge original dataframe with supervised features:\n",
    "df_supervised_merge= pq.read_table('Cohort/Feature_Extraction/ALL_HF_cohort_supervised_only_ever_diag_drugFORMerge_wLab.parquet').to_pandas()\n",
    "df_origin.index = df_origin.index.map(str)\n",
    "df_supervised_merge.index = df_supervised_merge.index.map(str)\n",
    "sup_colums=df_supervised_merge.columns\n",
    "df_cohort=pd.merge(df_origin, df_supervised_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "#merge with inpatient \n",
    "#df_inpatient_merge= pq.read_table('Cohort/Feature_Extraction/Supervised_ALL_HF/inpatient_events_merge.parquet').to_pandas()\n",
    "df_cohort.index = df_cohort.index.map(str)\n",
    "#df_inpatient_merge.index = df_inpatient_merge.index.map(str)\n",
    "#inp_colums=df_inpatient_merge.columns\n",
    "#df_cohort=pd.merge(df_cohort, df_inpatient_merge, left_on='medical_record_number', right_on='medical_record_number')\n",
    "#keep Patient with Time Series: \n",
    "df_cohort=df_cohort[df_cohort.index.isin(mrn)]\n",
    "# add cluster to Patients: \n",
    "df_cohort['LSTM_labels']=labels\n",
    "#df_cohort\n",
    "evaluation_results=[]\n",
    "#get general cluster information\n",
    "cluster_information=get_cluster_information(df_cohort)\n",
    "evaluation_results.append(cluster_information)\n",
    "#ANOVA \n",
    "#load one ctransformer:\n",
    "with open('Cohort/Models/ColumnTransformer/ALL_HF_cohort_unsupervised_only_after_onset_HF_ALL_all_any_all_mean_medium_cleaned.parquet_MinMaxScaler_BinaryEncoder.pkl', 'rb') as f:\n",
    "            ctransformer = pickle.load(f)\n",
    "ctransformer\n",
    "top_numerical_features_anova=num_feature_importance_anova(df_cohort,ctransformer,'LSTM_labels',n_cluster,5)\n",
    "print('Top Numerical features: \\n',top_numerical_features_anova)\n",
    "evaluation_results.append(top_numerical_features_anova)\n",
    "evaluation_results\n",
    "top_catigorical_features=cat_feature_importance(df_cohort,ctransformer,sup_colums,'LSTM_labels',n_cluster,5)\n",
    "print('Top Categorical features: \\n',top_catigorical_features)\n",
    "evaluation_results.append(top_catigorical_features)\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTopFeatures(df_cohort,True,'LSTM_labels', evaluation_results,n_cluster , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure='per_patient'#'per_patient_per_day'\n",
    "timesteps=len(data[0])\n",
    "n_features=len(data[0][0])\n",
    "layer_size_1=32\n",
    "layer_size_2=16\n",
    "activation_func='sigmoid'\n",
    "optimizer_func='adam'\n",
    "loss_func='mse'\n",
    "n_epochs=30\n",
    "n_batch_size=10\n",
    "X=sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['data_structure','timesteps','n_features','layer_size_1','layer_size_2','activation_func','optimizer_func','loss_func','n_epochs','n_batch_size','loss']\n",
    "result=pd.DataFrame(columns=col)\n",
    "result.to_parquet('Cohort/Metrics_LSTM.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pq.read_table('Cohort/Metrics_LSTM.parquet').to_pandas()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
